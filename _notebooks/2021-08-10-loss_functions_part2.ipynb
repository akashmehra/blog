{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93a6d18c",
   "metadata": {},
   "source": [
    "# Loss Functions Part 2\n",
    "\n",
    "> \"In this part of the multi-part series on the loss functions we'll be taking a. look at MSE, MAE, Hinge Loss, Triplet Loss and Huber Loss. We'll also look at the code for these Loss functions in PyTorch and some examples of how to use them\"\n",
    "\n",
    "- toc: true\n",
    "- branch: fastbook/lessons\n",
    "- badges: true\n",
    "- comments: true\n",
    "- image: images/triplet_loss.png\n",
    "- categories: [loss_functions]\n",
    "- hide: false\n",
    "- author: Akash Mehra"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac96e8f",
   "metadata": {},
   "source": [
    "## Mean Squared Error\n",
    "\n",
    "Mean Squared Error (MSE) measures the average of the squares of the errors for an estimator. The MSE of an estimator $\\hat{\\theta}$​ wrt an unknown parameter $\\theta$​ is defined as $MSE(\\hat{\\theta})  =E[(\\hat{\\theta} - \\theta)^2]$​​​. It can also be written as the sum of  *Variance* and the squared *Bias* of the estimator. In case of an *unbiased* estimator the *MSE* and *variance* are equivalent. Below is a quick way to see how the MSE and the Variance and Bias are related to each other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affcb7bb",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{split}\n",
    "E[X] & = \\mu\\\\\n",
    "\\end{split}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b872d62c",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{split}\n",
    "Var(X) & = E[(X - \\mu)^2]\\\\\n",
    "&= E[X^2] + \\mu^2 - 2\\mu E[X] \\\\\n",
    "&= E[X^2] - E[X]^2\n",
    "\\end{split}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e47844",
   "metadata": {},
   "source": [
    "using the following result:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3966bc7c",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{split}\n",
    "Var(X-Y) &= Var(X) + Var(Y) - 2Cov(X,Y)\\\\\n",
    "Var(\\mu) &= 0\\\\\n",
    "Cov(X,\\mu) &= E[X-E[X]]E[\\mu-E[\\mu]] = 0\\\\\n",
    "Var(X-\\mu) &= Var(X)\n",
    "\\end{split}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766d3d0f",
   "metadata": {},
   "source": [
    "We arrive at:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1000ddfd",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{split}\n",
    "MSE(\\hat{\\theta}) &= E[(\\hat{\\theta} - \\theta)^2] \\\\\n",
    "&= Var(\\hat{\\theta} - \\theta) + E[\\hat{\\theta} - \\theta]^2\\\\\n",
    "&= Var[\\hat{\\theta}] + Bias^2[\\hat{\\theta}]\n",
    "\\end{split}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3aa183",
   "metadata": {},
   "source": [
    "\n",
    "We can see from above that as *Variance,* *MSE* alwso heavily weights the outliers. It's squaring each term which weights large error terms more heavily than smaller ones. An alternative in that becomes *Mean Absolute Error* which is the topic of discussion of next section."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
