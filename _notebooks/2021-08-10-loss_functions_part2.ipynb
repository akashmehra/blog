{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93a6d18c",
   "metadata": {},
   "source": [
    "# Loss Functions Part 2\n",
    "\n",
    "> \"In this part of the multi-part series on the loss functions we'll be taking a. look at MSE, MAE, Hinge Loss, Triplet Loss and Huber Loss. We'll also look at the code for these Loss functions in PyTorch and some examples of how to use them\"\n",
    "\n",
    "- toc: true\n",
    "- branch: fastbook/lessons\n",
    "- badges: true\n",
    "- comments: true\n",
    "- image: images/triplet_loss.png\n",
    "- categories: [loss_functions]\n",
    "- hide: false\n",
    "- author: Akash Mehra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e0769a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#collapse-hide\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "precision=4\n",
    "np.set_printoptions(precision=precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae426072",
   "metadata": {},
   "source": [
    "In this post, I'd like to ensure that we're able to code the loss classes ourselves as well, that means that we'll need to be able to:\n",
    "\n",
    "1. Translate the equations to Python code for `forward` pass.\n",
    "2. Know how to calculate the gradients for a few loss functions, so that when we call `backward` pass the gradients get accumulated.\n",
    "\n",
    "Note that in presence of `autodiff` or `autograd` we won't need to do such a thing and this is purely for learning perspective, better implementations are offered by Deep Learning Libraries like `PyTorch`, `Tensorflow` etc.\n",
    "\n",
    "So, we'll define a class called `Module` that we'll use and override its `forward` and `backward` *abstract* functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "af7eded5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "\n",
    "class Module(ABC):\n",
    "    def __init__(self, reduction='mean'):\n",
    "        self.grad_input = None\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def __call__(self, input, target):\n",
    "        return self.forward(input, target)\n",
    "    \n",
    "    @abstractmethod\n",
    "    def forward(*args, **kwargs):\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def backward(*args, **kwargs):\n",
    "        pass\n",
    "    \n",
    "    def update_grad_input(self, grad):\n",
    "        if self.grad_input is not None:\n",
    "            self.grad_input += grad\n",
    "        else:\n",
    "            self.grad_input = grad\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        if self.grad_input is not None:\n",
    "            self.grad_input = np.zeros_like(self.grad_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea77d989",
   "metadata": {},
   "source": [
    "After having setup our *base* class, we can now look at individual Loss functions and start coding them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac96e8f",
   "metadata": {},
   "source": [
    "## Mean Squared Error\n",
    "\n",
    "Mean Squared Error (MSE) measures the average of the squares of the errors for an estimator. The MSE of an estimator $\\hat{\\theta}$​ wrt an unknown parameter $\\theta$​ is defined as $MSE(\\hat{\\theta})  =E[(\\hat{\\theta} - \\theta)^2]$​​​. It can also be written as the sum of  *Variance* and the squared *Bias* of the estimator. In case of an *unbiased* estimator the *MSE* and *variance* are equivalent. Below is a quick way to see how the MSE and the Variance and Bias are related to each other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affcb7bb",
   "metadata": {},
   "source": [
    "$$\n",
    "E[X] = \\mu\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002a556d",
   "metadata": {},
   "source": [
    "$$\n",
    "Var(X) = E[(X - \\mu)^2] = E[X^2] + \\mu^2 - 2\\mu E[X] \\implies E[X^2] - E[X]^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e47844",
   "metadata": {},
   "source": [
    "using the following result:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c0eac9",
   "metadata": {},
   "source": [
    "$$\n",
    "Var(X-Y) = Var(X) + Var(Y) - 2Cov(X,Y)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac5fefc",
   "metadata": {},
   "source": [
    "$$\n",
    "Var(\\mu) = 0\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3966bc7c",
   "metadata": {},
   "source": [
    "$$\n",
    "Cov(X,\\mu) = E[X-E[X]]E[\\mu-E[\\mu]] = 0 \\implies Var(X-\\mu) = Var(X)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766d3d0f",
   "metadata": {},
   "source": [
    "We arrive at:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f9a924",
   "metadata": {},
   "source": [
    "$$\n",
    "MSE(\\hat{\\theta}) = E[(\\hat{\\theta} - \\theta)^2] \\implies Var(\\hat{\\theta} - \\theta) + E[\\hat{\\theta} - \\theta]^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1000ddfd",
   "metadata": {},
   "source": [
    "$$\n",
    "MSE(\\hat{\\theta}) =  Var[\\hat{\\theta}] + Bias^2[\\hat{\\theta}]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3aa183",
   "metadata": {},
   "source": [
    "\n",
    "We can see from above that as *Variance,* *MSE* also heavily weights the outliers. It's squaring each term which weights large error terms more heavily than smaller ones. An alternative in that becomes *Mean Absolute Error* which is the topic of discussion of next section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af99b00",
   "metadata": {},
   "source": [
    "### MSE Loss in numpy\n",
    "\n",
    "We're using our implementation of the `Module` class. We'll define `forward` and `backward` functions for *MSE* Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "be740904",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSELoss(Module):\n",
    "    def __init__(self, reduction='mean'):\n",
    "        super(MSELoss, self).__init__(reduction)\n",
    "    \n",
    "    def forward(self, input, target):\n",
    "        red = getattr(np, self.reduction)\n",
    "        return np.asarray([red((target-input)**2),])\n",
    "    \n",
    "    def backward(self, input, target):\n",
    "        N = np.multiply.reduce(input.shape)\n",
    "        grad = (2/N * (input-target))\n",
    "        self.update_grad_input(grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53042251",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduction = 'mean'\n",
    "SEED=42\n",
    "rs = np.random.RandomState(SEED)\n",
    "input = rs.randn(3,2,4,5)\n",
    "target = rs.randn(3,2,4,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fab268f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.7061])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse = MSELoss(reduction)\n",
    "loss = mse(input, target)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54b9d937",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse.backward(input, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c7a924",
   "metadata": {},
   "source": [
    "Let's test our implementation against *PyTorch* and see if we've implemented things correctly or not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a34a47",
   "metadata": {},
   "source": [
    "### MSE Loss in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda135f6",
   "metadata": {},
   "source": [
    "Using the same input and target `numpy.ndarray` as `torch.tensor`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e392846d",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss(reduction=reduction)\n",
    "inp = Variable(torch.from_numpy(input), requires_grad=True)\n",
    "loss = criterion(inp, torch.from_numpy(target))\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "72701aa4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(mse.grad_input, inp.grad.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ae3c7a",
   "metadata": {},
   "source": [
    "## Mean Absolute Error Loss (MAE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec757c8",
   "metadata": {},
   "source": [
    "As pointed out earlier the *MSE* Loss suffers in the presence of outliers and heavily weights them. *MAE* on the other hand is more robust in that scenario. It is defined as the average of the absolute differences between the predictions and the target values.\n",
    "\n",
    "$$\n",
    "MAE(\\hat{\\theta}) = (E[|\\hat{\\theta} - \\theta|])\n",
    "$$\n",
    "\n",
    "It is also knows as the *L1Loss* as it is measuring the *L1* distance between two *vectors*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef3c359",
   "metadata": {},
   "source": [
    "### MAE/L1 Loss in numpy\n",
    "\n",
    "We'll use our implementation of the Module here as well and define the `forward` and `backward` function for `L1Loss`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "29d6375a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class L1Loss(Module):\n",
    "    def __init__(self, reduction='mean'):\n",
    "        super(L1Loss, self).__init__(reduction)\n",
    "    \n",
    "    def forward(self, input, target):\n",
    "        red = getattr(np, self.reduction)\n",
    "        return np.asarray([red((np.abs(target-input))),])\n",
    "    \n",
    "    def backward(self, input, target):\n",
    "        N = np.multiply.reduce(input.shape)\n",
    "        diff = input - target\n",
    "        mask_lg = (diff > 0) * 1.0\n",
    "        mask_sm = (diff < 0) * -1.0\n",
    "        mask_zero = (diff == 0 ) * 0.0\n",
    "        grad = 1/N * (mask_lg + mask_sm + mask_zero)\n",
    "        self.update_grad_input(grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1d80c014",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduction = 'mean'\n",
    "SEED=42\n",
    "rs = np.random.RandomState(SEED)\n",
    "input = rs.randn(3,2,4,5)\n",
    "target = rs.randn(3,2,4,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b4da965e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.0519])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l1loss = L1Loss(reduction)\n",
    "l1loss(input, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "beb0f602",
   "metadata": {},
   "outputs": [],
   "source": [
    "l1loss.backward(input, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e761da",
   "metadata": {},
   "source": [
    "As before, let's test our implementation against *PyTorch* and see if we've implemented things correctly or not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d944cd7",
   "metadata": {},
   "source": [
    "### MAE/L1 Loss in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f41f4f9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0519, dtype=torch.float64, grad_fn=<L1LossBackward>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion = nn.L1Loss(reduction='mean')\n",
    "inp = Variable(torch.from_numpy(input), requires_grad=True)\n",
    "loss = criterion(inp, torch.from_numpy(target))\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "afb1be7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a6ac15af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(l1loss.grad_input, inp.grad)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
