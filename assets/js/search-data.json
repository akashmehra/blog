{
  
    
        "post0": {
            "title": "Optimization  Notes",
            "content": "import numpy as np from mpl_toolkits.mplot3d import Axes3D import matplotlib.pyplot as plt fig = plt.figure(figsize=(10,8)) ax = fig.add_subplot(111, projection=&#39;3d&#39;) plot_args = {&#39;rstride&#39;: 1, &#39;cstride&#39;: 1, &#39;cmap&#39;:&quot;Blues_r&quot;, &#39;linewidth&#39;: 0.4, &#39;antialiased&#39;: True, &#39;vmin&#39;: -1, &#39;vmax&#39;: 1} x, y = np.mgrid[-1:1:31j, -1:1:31j] z = x**2 - y**2 ax.plot_surface(x, y, z, **plot_args) ax.plot([0], [0], [0], &#39;ro&#39;) ax.grid() plt.axis(&#39;off&#39;) ax.set_title(&quot;Saddle Point&quot;) plt.show() . . Image Credit: By Nicoguaro - Own work, CC BY 3.0, https://commons.wikimedia.org/w/index.php?curid=20570051 Optimization basics . Univariate Optimality Conditions: Quick recap . Consider a function $f(x)$​, where $x$​ is univariate, the necessary conditions for a point $x=x_0$​ to be a minimum of $f(x)$​ with respect to its infinitesimal locality are: $f&#39;(x_0) = 0$​ and $f&#39;&#39;(x_0) &gt; 0$​. The optimality conditions can be well undertsood if we look at the Taylor Series expansion of $f(x)$​ in the small vicinty of $x_0 + Delta$​. $$ f(x_0 + Delta) approx f(x_0) + Delta f&#39;(x) + frac{ Delta^2}{2} f&#39;&#39;(x) $$ Here, the value of $ Delta$​ is assumed to be very small. One can see that if $f&#39;(x) = 0$​ and $f&#39;&#39;(x) &gt; 0$​ then $f(x_0 + Delta) approx f(x_0) + epsilon$​​, which means that $f(x_0) &lt; f(x_0 + Delta)$​, for small values of $ Delta$​ (whether it is positive or negative) or $x_0$​ is a minimum wrt to its immediate locality. . Multivariate Optimality Conditions . Consider a function $f(x)$ where $x$ is an n-dimensional vector given by $ begin {bmatrix}x_1, x_2, x_3, cdots, x_n end {bmatrix}^T$. The gradient vector of $f(x)$ is . given by the partial derivatives wrt to each of the components of $x$, $ nabla {f{x}} equiv g(x) equiv begin {bmatrix} frac{ partial f}{ partial x_1} frac{ partial f}{ partial x_2} frac{ partial f}{ partial x_3} vdots frac{ partial f}{ partial x_n} end{bmatrix}$ . Note that the gradient in case of a multivariate functions is vector of n-dimensions. Similarly, one can define the second derivative of a multivariate function using a matrix of size $n times n$​ $$ nabla^2f(x) equiv H(x) equiv begin{bmatrix} frac{ partial^2 f}{ partial x_{1}^2} &amp; cdots &amp; frac{ partial^2 f}{ partial x_{1} partial x_{n}} vdots &amp; ddots &amp; vdots frac{ partial^2 f}{ partial x_{n} partial x_{1}} &amp; cdots &amp; frac{ partial^2 f}{ partial x_{n}^2} end{bmatrix} $$ Here, $H(x)$​​​​ is called a Hessian Matrix, and if the partial derivatives ${ partial^2 f}/{ partial x_{i} partial x_{j}}$​​​​​ and ${ partial^2 f}/{ partial x_{j} partial x_{i}}$​​​​​ are both defined and continuous and then by Clairaut&#39;s Theorem $ partial^2 f/ partial x_{i} partial x_{j}$​​​​​​​ = $ partial^2 f/ partial x_{j} partial x_{i}$​​​​​,​​ this second order partial derivative matrix becomes symmetric. . If $f$​ is quadratic then the Hessian becomes a constant, the function can then be expressed as: $f(x) = frac{1}{2}x^THx + g^Tx + alpha$​ , and as in case of univariate case the optimality conditions can be derived by looking at the Taylor Series expansion of $f$​ about $x_0$: $$ f(x_0 + epsilon overline{v}) = f(x_0) + epsilon overline{v}^T nabla f(x_0) + frac { epsilon^2}{2} overline{v}^T H(x_0 + epsilon theta overline{v}) overline{v} $$ where $0 leq theta leq 1$​, $ epsilon$​ is a scalar and $ overline{v}$​ is an n-dimensional vector. Now, if $ nabla f(x_0) = 0$​, then it leaves us with $f(x_0) + frac{ epsilon^2}{2} overline{v}^T H overline{v}$​, which implies that for the $x_0$​ to be a point of minima, $ overline{v}^T H overline{v}&gt; 0$​​ or the Hessian has to be positive definite. . Quick note on definiteness of the symmetric Hessian Matrix . $H$ is positive definite if $ mathbf{v}^TH mathbf{v} &gt; 0$, for all non-zero vectors $ mathbf{v} in mathbb{R}^n$ (all eigenvalues of $H$ are strictly positive) | $H$ is positive semi-definite if $ mathbf{v}^TH mathbf{v} geq 0$, for all non-zero vectors $ mathbf{v} in mathbb{R}^n$ (eigenvalues of $H$ are positive or zero) | $H$ is indefinite if there exists a $ mathbf{v}, mathbf{u} in mathbb{R}^n$, such that $ mathbf{v}^TH mathbf{v} &gt; 0$ and $ mathbf{u}^T H mathbf{u} &lt; 0$ (eigenvalues of $H$ have mixed sign) | $H$ is negative definite if $ mathbf{v}^TH mathbf{v} &lt; 0$, for all non-zero vectors $ mathbf{v} in mathbb{R}^n$ (all eigenvalues of $H$ are strictly negative) | . Gradient based optimization . Gradient based optimization is a technique to minimize/maximize the function by updating the paremeters/weights of a model using the gradients of the Loss wrt to the parameters. If the Loss function is denoted by $E(w)$​​​​​​​​, where $w$​​​​​​​​ are the parameters then we&#39;d like to calculate $ nabla_{w} E( mathbb{w})$​​​​​​​​ and to get the parameters for the next iteration, we&#39;d like to perform the $ mathbb{w}_{t+1} = mathbb{w}_{t} - eta * nabla_{ mathbb{w}} E( mathbb{w}_{t})$​​​​​, where $ eta$​​​​​​​​​ is the learning rate. . In order to update the parameters of a Machine Learning model we need a way for us to measure the rate of change in the output when the inputs (the parameters) are changed. . Here, we&#39;ve assumed that the function $E( mathbb{w})$​​​​ is continuous. The function $E( mathbb{w})$​​​ can have kinks in which case we&#39;ll call the gradient a subgradient. Subgradient generalizes the notion of a derivative to functions that are not necessirily differentiable. More on subgradients will follow in a separate post, but for now assume that there exists a concept using which one can calculate the gradient of a function that is not differentiable everywhere (has kinks, for example:ReLU non-linearity). There are many gradient based optimization algorithms that exist and they differ mainly in how the gradients are calculated or how the learning rate $ eta$​​ is chosen. We&#39;ll look at some of those algorithms that are used in practice. . Need for Gradient Descent . One of the necessary conditions for a point to be a critical point (minima, maxima or saddle) is that the first order derivate $f&#39;(x) = 0$​, it is often the case that we&#39;re not able to exactly solve this equation because the derivative can be a complex function of $x$​. A closed form solution so to speak, doesn&#39;t exist and things get even more complicated in multivariate case due to compultational and numerical challenges[1]. We use Gradient Descent to iteratively solve the optimization problem irrespective of the functional form of $f(x$​) by taking a step in the direction of the steepest descent (because in Machine Learning we&#39;re optimizing a Loss or a Cost function, we tend to always solve the optimization problem from the perspective of minimization). . Convergence of Gradient Descent . We also need to talk about the convergence of Gradient Descent before we can dive into different types of gradient based methods. Let&#39;s look at the update equation once again: $ mathbb{w}_{t+1} = mathbb{w}_{t} - eta frac{ partial E( mathbb{w})}{ partial mathbb{w}}$​​​​​​, to see the effect of learning rate in univariate case let&#39;s take a look athe following figures: . . Source: Efficient Backprop, Lecun et al. 98 $ eta_{opt}$​​​ is the optimal learning rate, and we can see in a) if our chosen learning rate $ eta &lt; eta_{opt}$​​​ then converges will happen at a slower pace, in (b we see that when $ eta = eta_{opt}$​​​ then we just converge right away and for $ eta_{opt} &lt; eta &lt; 2 eta_{opt}$​​​ the weights oscilate around the minimum but evenbtually converge. Things get difficult in case where $ eta &gt; 2 eta_{opt}$​​​​, when this happens, weights diverge. We also need to find out the value of $ eta_{opt}$​​​, and to do so we need to write out the Taylor Series expansion of our function $E( mathbb{w})$​​​ about current weight $ mathbb{w}_{c}$​. As we know from Section 1.2​, we could expand our function as: $$ E( mathbb{w}) = E( mathbb{w}_{c}) - ( mathbb{w} - mathbb{w}_{c}) frac { partial E( mathbb{w})}{ partial mathbb{w}} + frac{1}{2} ( mathbb{w} - mathbb{w}_{c})^2 frac { partial^2 E( mathbb{w})}{ partial mathbb{w}^2} + cdots, $$ as before, if $E( mathbb{w})$​ is quadratic then we&#39;re left with only the first and second order terms. Differentiating both sides wrt w and noting that higher order terms will vanish as the second order derivative itself is a constant, we&#39;re left with: $$ frac { partial E( mathbb{w})}{ partial mathbb{w}} = frac { partial E( mathbb{w_{c}})}{ partial mathbb{w}} + ( mathbb{w} - mathbb{w}_{c}) frac { partial^2 E( mathbb{w})}{ partial mathbb{w}^2} $$ Now setting $ mathbb{w} = mathbb{w}_{min}$​​ and noting that $ frac { partial E( mathbb{w}_{min})}{ partial mathbb{w}} = 0$, we get $$ ( mathbb{w}_{c} - mathbb{w}_{min}) frac { partial^2 E( mathbb{w})}{ partial mathbb{w}^2} = frac { partial E( mathbb{w_{c}})}{ partial mathbb{w}} implies boxed { mathbb{w}_{min} = mathbb{w}_{c} - left( frac { partial^2 E( mathbb{w})}{ partial mathbb{w}^2} right)^{-1} frac { partial E( mathbb{w_{c}})}{ partial mathbb{w}}} $$ The boxed equation looks a lot familiar, it turns out that it&#39;s our weight update equation which tells us that we can reach the minimum in one step if we set $ eta_{opt} = left( frac { partial^2 E( mathbb{w})}{ partial mathbb{w}^2} right)^{-1} $​, one extending this multivariate case we get $ eta_{opt} = H^{-1}( mathbb{w})$​​. . This takes us to the Newton based methods (a type of gradient based optimization). Note how we don&#39;t have to rey on a hyperparameter like the learning rate if we could somehow compute the inverse of the Hessian matrix. Multiplying the gradient vector with the inverse of the Hessian takes smaller steps in the direction of steep curvatire but takes larger steps in the direction of shallow curvature. Although, in theory it sounds nice but it&#39;s often very hard to compute the inverse of the Hessian for most practical situations. Algorithms like L-BFGS exist that reduce the memory requirements needed for Newton based methods but in practice we hardly see them applied in training Deep Neural Networks. . For more on the convergence theory, please read Section 5 of Efficient Backprop by Yann LeCun. . Gradient Descent and its variants . in this section I&#39;ll cover a few common variants of Gradient Descent that are most commonly used in practice, a more comprehensive treatment on the topic is convered in [2] and it&#39;s an amazing text to refer. . Stochastic Gradient Descent (SGD) . The variant of Gradient Descent that we&#39;ve seen so far is commonly known as Batched Gradient Descent, which requires us to calculate the gradients at each step by considering the whole dataset. As an alternate one can just use one example chosen at random from the whole dataset to calculate the gradient as well. This gradient albiet noisy, leads to better solutions. In code it looks something like this . for epoch in range(num_epochs): np.random.shuffle(data) for x,y in data: grads = eval_grads(loss, params, x, y) params -= learning_rate * grads . Here, the tuple x,y is one example, label pair samped from the dataset. Note that as previously mentioned, the updates performed using one example are very noisy. A graph of SGD fluctuation is follows: . Image Credit: Wikipedia With Batched Gradient Descent, it converges to the minima of the basin where the weights were initialized, whereas with Stochastic Gradient Descent, due to the noisy nature of the updates, the weights can potentially jump out of the basin and find a better minima. One thing to note is that this noise can make the convergence a lot slower as well as the weights can keep overshooting the minima, but it has been shown that on slowly annealing the learning rate, one can converge to a minima. Please see [1] . Mini-batch Gradient Descent . Mini Batch Gradient Descent is a good middle ground between the more expensive Batched version and the noisier Stochastic version of Gradient Descent. In this version, instead of sampling one exa,mple at random at a time, we sample a mini batch of a pre-defined size (which is a hyper-parameter). The benefit of this method is that it can help reduce the variance of the parameter updates (there by making the updates less noisy and having less oscialltions), it can also help us leverage the state of the art Deep Learning software libraries that have an efficient way of calculating radients for mini batches . Again, in code it looks something like this: . for epoch in range(num_epochs): batch_generator = BatchGenerator(batch_size=64) for batch in batch_generator: grads = eval_grads(loss, params, batch) params -= learning_rate * params . As pointed by [2], choosing the right learning rate can be very difficult, and even though learning rate schedules can help (annealing the learning rate using a pre-defined schedule), these schedules don&#39;t adapt as the training prgresses. . In 2015 the researcher Leslie Smith came up with the learning rate finder. The idea was to start with a very, very small learning rate and use that for one mini-batch, find what the losses are afterwards, and then increase the learning rate by some percentage (e.g., doubling it each time). Then take a pass over another mini-batch, track the loss, and double the learning rate again. This is done until the loss gets worse, instead of better. . SGD with Momentum . Momentum is one way to help SGD converge faster by accelrating in the relevant directions and reducing the oscillations (which are a result of gradients of components point in different directions). . Exponentially Moving Averages . Before we build up the equations for momentum based updates in SGD let&#39;s do a quick review of Exponentially weighted averages. Given a signal, one can compute the Exponentially weighted average given a parameter $ beta$ using the following equation:$$ mathcal{V}_{t} = beta cdot mathcal{V}_{t-1} + (1- beta) cdot mathcal{ theta}_{t} $$ Where $ mathcal{ theta}_{t}$ is the current value of the signal. We can use plot this in python code as well: . from matplotlib import pyplot as plt import seaborn as sns import numpy as np from scipy import stats sns.set_theme() def get_signal(time): x_volts = 10*np.sin(time/(2*np.pi)) x_watts = x_volts ** 2 # Calculate signal power and convert to decibles sig_avg_watts = np.mean(x_watts) return x_volts, x_watts, 10 * np.log10(sig_avg_watts) def get_noise(sig_avg_db, target_snr_db): # Calculate noise according to SNR = P_signal - P_noise # then convert to watts noise_avg_db = sig_avg_db - target_snr_db noise_avg_watts = 10 ** (noise_avg_db / 10) return 0, noise_avg_watts def moving_average(data, beta=0.9): v = [0] for idx, value in enumerate(data): v.append(beta * v[idx] + (1-beta) * value) return np.asarray(v) t = np.linspace(1, 20, 100) # Set a target SNR target_snr_db = 12 x_volts, x_watts, sig_avg_db = get_signal(t) mean_noise, noise_avg_watts = get_noise(sig_avg_db, target_snr_db) noise_volts = np.random.normal(mean_noise, np.sqrt(noise_avg_watts), len(x_volts)) # Noise up the original signal y_volts = x_volts + noise_volts plt.style.use(&#39;fivethirtyeight&#39;) # Plot signal with noise fig, ax = plt.subplots(figsize=(10,6)) for beta, color in [(0.5, &#39;black&#39;), (0.9, &#39;blue&#39;), (0.98,&#39;darkgreen&#39;)]: y_avg = moving_average(y_volts, beta=beta) ax.plot(t, y_avg[1:], label=f&#39;beta={beta}&#39;, color=color, linewidth=1.5) ax.scatter(t, y_volts, label=&#39;signal with noise&#39;, color=&#39;orange&#39;, marker=&#39;s&#39;) ax.legend(loc=&#39;upper left&#39;) ax.grid(False) plt.title(&#39;Exponentially Moving Average&#39;, fontsize=15) plt.ylabel(&#39;Signal Value&#39;, fontsize=12) plt.xlabel(&#39;Time&#39;, fontsize=12) plt.show() . . We can see the effect of the variable $ beta$​ on the smoothness of the curve, the green curve corresponding to $ beta = 0.98$​ is smoother and shifted right because it&#39;s slow to account for the change in the current signal value, recall that the equation for the Exponentially Moving Average is: $ mathcal{V}_{t} = beta cdot mathcal{V}_{t-1} + (1- beta) cdot mathcal{ theta}_{t}$​ and a higher $ beta$​ means that we&#39;re paying more attention to past values than the current one. . Now what does this have to do with SGD or Momentum? . The parameter update equation for SGD with Momentum is as follows:$$ mathcal{V}_{t} = beta cdot mathcal{V}_{t-1} + (1- beta) cdot nabla_{ mathbb{w}} E( mathbb{w}_{t}) $$ . $$ mathbb{w}_{t+1} = mathbb{w}_{t} - eta * mathcal{V}_{t} $$In some implementations we just omit the $1- beta$ term and simply use the following: . $$ mathcal{V}_{t} = beta cdot mathcal{V}_{t-1} + eta cdot nabla_{ mathbb{w}} E( mathbb{w}_{t}) $$$$ mathbb{w}_{t+1} = mathbb{w}_{t} - mathcal{V}_{t} $$where, $ mathcal{V}_{t}$ is the Exponentially Moving Average. . With momentum update like above, the parameters will build up the velocity in direction that has consistent gradient. The variable $ beta$​ can be interpreted as coefficient of friction which has a dampening effect on an object moving at a certain velocity. This variable reduces the Kinetic Energy of the system, which would otherwise never come to a stop. . Nesterov Momentum . The idea behind Nesterov Momentum is to lookahead at the value of the weights in the basin where we&#39;ll end up at if we had applied the momentum update. So, instead of using $ mathbb{w}_{t}$​​ we apply the momentum update to calculate $ mathbb{w}_{t}^{ahead} = mathbb{w}_{t} - beta cdot mathcal{V}_{t} $​​ and use this value to calculate the gradient and perform the update, reason being that we&#39;re going to land in the vicity of this point anyway, and being approximately there gives us a better estimate of the updates. . $$ mathbb{w}_{t}^{ahead} = mathbb{w}_{t} - beta cdot mathcal{V}_{t} $$$$ mathcal{V}_{t} = beta cdot mathcal{V}_{t-1} + eta cdot nabla_{ mathbb{w}} E( mathbb{w}_{t}^{ahead}) $$$$ mathbb{w}_{t+1} = mathbb{w}_{t} - mathcal{V}_{t} $$$$ $$ . Source: CS231n I&#39;ll stop here for now and talk about RMSProp, AdaGrad, Adam in a later post. . References . [1] Efficient Backprop by Yann LeCun . [2] An overview of gradient descent optimization algorithms . [3] Convolutional Neurak Networks for Visual Recognition . [4] C. Darken, J. Chang and J. Moody, &quot;Learning rate schedules for faster stochastic gradient search,&quot; Neural Networks for Signal Processing II Proceedings of the 1992 IEEE Workshop, 1992, pp. 3-12, doi: 10.1109/NNSP.1992.253713. . [5] Fastbook Chapter 5 . [6] Leslie N. Smith, Cyclical Learning Rates for Training Neural Networks, arXiv:1506.01186 . [7] Why Momentum Really Works . [8] Improving Deep Neural Networks: Hyperparameter Tuning, Regularization and Optimization . [9] Lecture Notes AA222 . [10] Linear Algebra and Optimization for Machine Learning . [11] Adding Noise to a signal in Python .",
            "url": "https://akashmehra.github.io/blog/optimization/2021/07/27/optimization.html",
            "relUrl": "/optimization/2021/07/27/optimization.html",
            "date": " • Jul 27, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "A comparison of Pets classifier using vanilla PyTorch and Fast.ai",
            "content": "Disclaimer: I am a fairly new to the library and this is just to show what I&#39;ve observed so far, I may be missing a point or two, so, don&#39;t treat the above as a comprehensive list of what fastai can do. This is just my attempt to keep learning and evolving. . Introduction . Below is generally the plan that everyone follows when it comes to training a Machine Learning model: . Load Data | Inspect Data: Plot a few examples | Create a DataLoader | Define a model architecture. | Write a training loop. | Plot metrics. | . There&#39;s also another step, which is . Analyze Errors. | . but we&#39;ll tackle this in a separate blog post once we&#39;ve covered the training of the model bit. . PyTorch Version . from torch import nn from torch import optim from torch.utils.data import dataset, dataloader from torch.autograd import Variable from torch.nn import functional as F from torchvision import datasets, transforms from torchvision.models.resnet import resnet18 from tqdm.notebook import tqdm from sklearn.model_selection import train_test_split import os from collections import Counter, OrderedDict import re import requests import tarfile . . Fetch Data . The data is a tar-gzip archive, to extract data files from it we&#39;ll use the package called tarfile. But first we need to download the archive. . The code in the cell below is taken form: https://gist.github.com/devhero/8ae2229d9ea1a59003ced4587c9cb236#gistcomment-3775721. . def fetch_data(url, data_dir, download=False): if download: response = requests.get(url, stream=True) file = tarfile.open(fileobj=response.raw, mode=&quot;r|gz&quot;) file.extractall(path=data_dir) . In the interest of comparison, I&#39;ll first write the Dataset class and see how easy it gets when we use fastai. The url that we want to fetch the data from is here: Pets Dataset . pets_url = &#39;https://s3.amazonaws.com/fast-ai-imageclas/oxford-iiit-pet.tgz&#39; data_dir = os.path.join(&#39;gdrive&#39;, &#39;MyDrive&#39;, &#39;pets_data&#39;) base_img_dir = os.path.join(data_dir, &#39;oxford-iiit-pet&#39;, &#39;images&#39;) fetch_data(pets_url, data_dir) . . We&#39;ve extracted the data in the folder named pets_data, on inspection, it looks like the folder pets_data/oxford-iiit-pet/images contains all the images we want (some files need to be filtered out as they&#39;re not in JPEG format). The filenames have the category labels in their name itself in the format: &lt;CATEGORYNAME&gt;_&lt;NUMBER&gt;.jpg. . Extract Labels . In order to extract the category name from the file names in fastbook, a RegexLabeller is used. We&#39;ll write a similar LabelExtractor (although it&#39;s very inferior in functionality to the fastai&#39;s RegexLabeller but does the job for now). . class RegexLabelExtractor(): def __init__(self, pattern): self.pattern = pattern self._names = [] def __call__(self, iterable): return [re.findall(self.pattern, value)[0] for value in iterable] . As mentioned before our version, RegexLabelExtractor extracts the label given a text. It accepts a pattern during instantiation and on __call__ it expects an iterable containing a list of texts containing the labels. It returns all the label names in a Python list . Once we&#39;ve defibed a class for extracting the labels, we&#39;d like to define a container that is responsible for maintaining a map of CATEGORYNAME -&gt; ID, which we&#39;ll use to convert the labels to an integer format and vice versa. . Below we define a LabelManager, it exposes a id_for_label* and label_for_id* methods along with keys, which returns the unique label names in our dataset (this our vocabulary size). We can also call len on a LabelManager object to know the number of output classes. . *These are a type of OrderedDict. . class LabelManager(): def __init__(self, labels): self._label_to_idx = OrderedDict() for label in labels: if label not in self._label_to_idx: self._label_to_idx[label] = len(self._label_to_idx) self._idx_to_label = {v:k for k,v in self._label_to_idx.items()} @property def keys(self): return list(self._label_to_idx.keys()) def id_for_label(self, label): return self._label_to_idx[label] def label_for_id(self, idx): return self._idx_to_label[idx] def __len__(self): return len(self._label_to_idx) . Data Splitter . We&#39;d also like to spilt our dataset into train and validation subsets. Although the dataset provides a list of train and validation splits but to be consistent with the book, we&#39;ll just write our version of the RandomSplitter (which again would be very inferior in functionality, but will do the job for the purposes of demonstration). . We&#39;d like this Splitter to accept a percentage to split on and also a seed for reproducibility. . class Splitter(): def __init__(self, valid_pct=0.2, seed = None): self.seed = seed self.valid_pct = valid_pct def __call__(self, dataset): return train_test_split(dataset, test_size=self.valid_pct, random_state=np.random.RandomState(self.seed)) . Writing a PyTorch Dataset . Now that we have a way to extract labels, maintain them in a map and split the data into train and validation splits, we&#39;ll define a PetsDataset ( a PyTorch Dataset) which will be used by the PyTorch DataLoaderto give us the data we need to provide our model to train. . A note on PyTorch Dataset: A PyTorch dataset is a primitive provided by the library that stores the samples and their corresponding labels. In order to write a custom dataset, our class PetsDataset needs to implement three functions: __init__, __len__, and __getitem__. . class PetsDataset(dataset.Dataset): def __init__(self, data, tfms=None): super(PetsDataset, self).__init__() self.data = data self.transforms = tfms def __getitem__(self, idx): X = Image.open(self.data[idx][0]) if X.mode != &#39;RGB&#39;: X = X.convert(&#39;RGB&#39;) y = self.data[idx][1] if self.transforms: X = self.transforms(X) return (X, y) def __len__(self): return len(self.data) . Notice how we&#39;re opening the Image only when __getitem__ is called and we also have to make sure that all the images have 3 input channels, hence the check if X.mode != &#39;RGB&#39;. Some images in the dataset have this issue and if we don&#39;t convert them to have 3 input channels then the DataLoader wouldn&#39;t be able to create a batch using torch.stack . We&#39;re now ready to use these datasets, but we&#39;ll need to make sure that our global map of CATEGORYNAME -&gt; ID is constructed using both the train and the validation splits, we&#39;ll also have this class hold our corresponding datasets. . class DatasetManager(): def __init__(self, base_dir, paths, label_extractor, tfms=None, valid_pct=0.2, seed=None): self._labels = label_extractor(paths) self.tfms = tfms self._label_manager = LabelManager(self._labels) self._label_ids = [self.label_manager.id_for_label(label) for label in self._labels] self.abs_paths = [os.path.join(base_dir, path) for path in paths] self.train_data, self.valid_data = Splitter(valid_pct=valid_pct, seed=seed)(list(zip(self.abs_paths, self._label_ids))) @property def label_manager(self): return self._label_manager @property def train_dataset(self): return PetsDataset(self.train_data, tfms=self.tfms) @property def valid_dataset(self): return PetsDataset(self.valid_data, tfms=self.tfms) . We&#39;ll now use all the helper classes we&#39;ve created so far to use the datasets in a dataloader and look at the plan to choose an architecture and train it (almost there). . paths = [path for path in sorted(os.listdir(base_img_dir)) if path.endswith(&#39;.jpg&#39;)] pattern = &#39;(.+)_ d+.jpg$&#39; regex_label_extractor = RegexLabelExtractor(pattern) dataset_manager = DatasetManager(base_img_dir, paths, regex_label_extractor, tfms=transforms.Compose([transforms.Resize((224, 224)), transforms.ToTensor()]), seed=42) train_dataset = dataset_manager.train_dataset valid_dataset = dataset_manager.valid_dataset . Before we look at the model, let&#39;s just for sake of sanity look at the labels we&#39;re dealing with and possibly plot a few images. This step is just to make sure that things are working as expected and the dataloader will be batching the data in the right way and our Trainer won&#39;t crash midway. . df = pd.DataFrame(dataset_manager.label_manager.keys, columns=[&#39;label_name&#39;]) df.head(len(df)) . label_name . 0 Abyssinian | . 1 Bengal | . 2 Birman | . 3 Bombay | . 4 British_Shorthair | . 5 Egyptian_Mau | . 6 Maine_Coon | . 7 Persian | . 8 Ragdoll | . 9 Russian_Blue | . 10 Siamese | . 11 Sphynx | . 12 american_bulldog | . 13 american_pit_bull_terrier | . 14 basset_hound | . 15 beagle | . 16 boxer | . 17 chihuahua | . 18 english_cocker_spaniel | . 19 english_setter | . 20 german_shorthaired | . 21 great_pyrenees | . 22 havanese | . 23 japanese_chin | . 24 keeshond | . 25 leonberger | . 26 miniature_pinscher | . 27 newfoundland | . 28 pomeranian | . 29 pug | . 30 saint_bernard | . 31 samoyed | . 32 scottish_terrier | . 33 shiba_inu | . 34 staffordshire_bull_terrier | . 35 wheaten_terrier | . 36 yorkshire_terrier | . . Data Inspection . A method to plot one batch of data (inspired by fastai of course but again a very curtailed version of what that function does). Notice how we&#39;re calling transforms.ToPILImage(), that&#39;s because we have objects of type torch.Tensor in our batch and in order to plot them we need to convert them to a PIL.Image, rest everything is done to just make sure we&#39;ve got the images aligned in a nice way across different panels. . def plot_one_batch(batch, max_images=9): nrows = int(math.sqrt(max_images)) ncols = int(math.sqrt(max_images)) if nrows * ncols != max_images: nrows = (max_images + ncols - 1) // ncols fig, ax = plt.subplots(nrows=nrows, ncols=ncols, figsize=(20, 10)) X,Y = next(batch) for idx, x in enumerate(X[:max_images]): y = Y[idx] ax.ravel()[idx].imshow(transforms.ToPILImage()(x)) ax.ravel()[idx].set_title(f&#39;{y}/{dataset_manager.label_manager.label_for_id(y.item())}&#39;) ax.ravel()[idx].set_axis_off() plt.tight_layout() plt.show() . This function generates one batch of data given a dataloader, this is just using Python Generators . def generate_one_batch(dl): for batch in dl: yield batch . plot_one_batch(generate_one_batch(train_dl), max_images=20) . Model Architecture . Now, we&#39;re ready to look at the model and make a few decisions about the architecture we want to use. . Here&#39;s our requirement: We want to extract the features from an image and then uses a classification head to get the output distribution over the number of classes (our labels from before). We&#39;ll define a loss and use it to optimize the network. . Because we&#39;re dealing with images, a Convolution Neural Network (CNN) seems like a good start, in the literature as well as fastbook, a restnet type architecture has been used, so let&#39;s use that and see what we can do with it. . Coding a ResNet is a separate blog post on its own, so, we&#39;ll punt that for now and use what&#39;s available to us in the form a pretrained model. . model = resnet34(pretrained=True, progress=True) . Downloading: &#34;https://download.pytorch.org/models/resnet34-b627a593.pth&#34; to /root/.cache/torch/hub/checkpoints/resnet34-b627a593.pth . . Changing the classifier . Since this model is trained to give an output distribution for 1000 classes, we can just change that layer to give us an output distribution based on what we have in our dataset and then fine-tune this layer. To read more on fine-tuning refer to fastbook . model.fc = nn.Linear(512,len(dataset_manager.label_manager), bias=True) . Making the model fine-tunable . We&#39;ll freeze all the layers of the model except for the fc classification head we added above. . def make_fine_tunable(model): for param in model.parameters(): param.requires_grad = False for param in model.fc.parameters(): param.requires_grad = True print(&quot;Tunable Layers: &quot;) for (name, param) in model.named_parameters(): if param.requires_grad: print(f&#39;{name} -&gt; {param.requires_grad}&#39;) . make_fine_tunable(model) . Tunable Layers: fc.weight -&gt; True fc.bias -&gt; True . . Trainer . Now comes in a point where we have to write a traininig loop and this is where things get into the Boiler Plate code category even more. We shouldn&#39;t be writing this but that&#39;s the point of this blog post that using fastai we can offload a lot of the boiler plate code to the library and use the goodies offered by the library to our advantage and focus more on research/modeling. . We are maintaining an instance of model, criterion, an optimizer and dataloaders. We step through a batch during train_epoch and incur a loss. We use this loss to make a backward pass and let the optimizer take a step by updating the network parameters. We also have a validate function that calculates loss and accuracy on validation dataset after every epoch . class Trainer(): def __init__(self, train_dataloader, model, criterion, optimizer, test_dataloader=None): self.train_dl = train_dataloader self.model = model self.test_dl = test_dataloader self.criterion = criterion self.optimizer = optimizer self.recorder = {&#39;loss&#39;: { &#39;train&#39;: {}, &#39;test&#39;: {}} , &#39;accuracy&#39;: {&#39;train&#39;: {}, &#39;test&#39;: {}}} def step_batch(self, X,y): X = X.cuda() y = y.cuda() logits = self.model(X) loss = self.criterion(logits, y) probs = F.softmax(logits, dim=1) return loss, logits, probs def train_epoch(self, epoch): self.model.train() running_loss = 0 for X,y in tqdm(self.train_dl, leave=False): self.optimizer.zero_grad() loss, _, _ = self.step_batch(X,y) running_loss += loss loss.backward() self.optimizer.step() epoch_loss = running_loss / len(self.train_dl) self.recorder[&#39;loss&#39;][&#39;train&#39;][epoch] = epoch_loss return epoch_loss @torch.no_grad() def accuracy(self): correct = 0 total = 0 for X,y in tqdm(self.test_dl): total += y.size(0) logits = model(X) probs = F.softmax(logits, dim=1) _, y_pred = torch.max(probs, dim=1) correct += (y_pred == y).sum() acc = correct / float(total) return acc @torch.no_grad() def validate(self, epoch): running_loss = 0 total = 0 correct = 0 for X,y in tqdm(self.test_dl, leave=False): y = y.cuda() total += y.size(0) loss, logits, probs = self.step_batch(X,y) running_loss += loss _, y_pred = torch.max(probs, dim=1) correct += (y_pred == y).cpu().sum() acc = correct / float(total) epoch_loss = running_loss / len(self.test_dl) self.recorder[&#39;loss&#39;][&#39;test&#39;][epoch] = epoch_loss self.recorder[&#39;accuracy&#39;][&#39;test&#39;][epoch] = acc return epoch_loss, acc def train(self, num_epochs): for epoch in tqdm(range(num_epochs), leave=False): train_loss = self.train_epoch(epoch) test_loss, test_acc = self.validate(epoch) #print(f&quot;Training Loss: {train_loss}, tTest Loss: {test_loss}, tTest Accuracy: {test_acc}&quot;) . Training (fine-tuning) the model . Let&#39;s send the model over to the GPU for faster training. . model = model.cuda() . Hyperparameters . Let&#39;s define a configuration that will hold our hyper-parameters . class TrainConfig(): def __init__(self, bs=32, lr=1e-2, seed=42, betas=(0.9, 0.999), num_workers=4): self.bs = bs self.lr = lr self.seed = seed self.betas = betas self.num_workers = num_workers . We set the seed for reproducibility, and instantiate dataloader objects. Notice how we&#39;re using &gt; 1 num_workers. That speeds up the data loading process. . config = TrainConfig(bs=128) torch.manual_seed(config.seed) train_dl = dataloader.DataLoader(train_dataset, batch_size=config.bs, shuffle=True, num_workers=config.num_workers) valid_dl = dataloader.DataLoader(valid_dataset, batch_size=config.bs, shuffle=False, num_workers=config.num_workers) . We define our criterion as nn.CrossEntropy and choose our optimizer to be an instance of optim.Adam, after that we instantiate our trainer object and train (fine-tune in our case) for a few epochs. . Criterion, Optimizer and Training . criterion = nn.CrossEntropyLoss() optimizer = optim.Adam(model.parameters(), lr=1e-3, betas=(0.9, 0.999)) trainer = Trainer(train_dl, model, criterion, optimizer, test_dataloader=valid_dl) trainer.train(10) . Plotting Utilities . Helper functions for plotting our loss and accuracies (which we&#39;ve recorded using our trainer) . def plot_losses(losses): train_loss = losses[&#39;train&#39;] test_loss = losses[&#39;test&#39;] plt.style.use(&#39;fivethirtyeight&#39;) fig, ax = plt.subplots(figsize=(7, 4)) ax.plot(train_loss, color=&#39;blue&#39;, label=&#39;Training Loss&#39;) ax.plot(test_loss, color=&#39;green&#39;, label=&#39;Test Loss&#39;) ax.set(title=&quot;Loss over epochs&quot;, xlabel=&quot;Epochs&quot;, ylabel=&quot;Loss&quot;) ax.legend() fig.show() plt.style.use(&#39;default&#39;) . def plot_accuracy(accuracy): plt.style.use(&#39;fivethirtyeight&#39;) fig, ax = plt.subplots(figsize=(7, 4)) ax.plot(accuracy, color=&#39;blue&#39;, label=&#39;Test Accuracy&#39;) ax.set(title=&quot;Accuracy over epochs&quot;, xlabel=&quot;Epochs&quot;, ylabel=&quot;Accuracy&quot;) ax.legend() fig.show() plt.style.use(&#39;default&#39;) . Loss . losses = { k: np.asarray([t.item() for t in v.values()]) for k,v in trainer.recorder[&#39;loss&#39;].items() } plot_losses(losses) . Accuracy . accuracies = { k: np.asarray([t.item() for t in v.values()]) for k,v in trainer.recorder[&#39;accuracy&#39;].items()} plot_accuracy(accuracy=accuracies[&#39;test&#39;]) . Summary of PyTorch Version . Load Data | Inspect Data: Plot a few examples | Create a DataLoader | Define a model architecture. | Write a training loop. | Plot metrics. | Fast.ai Version . Now let&#39;s see how this can done using fastai. One could treat the following cells as a completely different notebook altogether. . from fastcore.all import L from fastai.vision.all import * matplotlib.rc(&#39;image&#39;, cmap=&#39;Greys&#39;) . . Fetch Data . We&#39;ll first download the Pets data and untar it using the untar_data function and this function really takes care of filtering the images and storing them somewhere on the disk for us and then returning the paths. It&#39;s helpful as I don&#39;t have to take a peek at the response object and parse it then untar it, apply filters and then iterate through the directory. This function does it all for us. To know more about untar_data, please checkout the documentation for untar_data . path = untar_data(URLs.PETS) . Path.BASE_PATH = path path.ls() . (#2) [Path(&#39;annotations&#39;),Path(&#39;images&#39;)] . . (path/&quot;images&quot;).ls() . (#7393) [Path(&#39;images/great_pyrenees_160.jpg&#39;),Path(&#39;images/shiba_inu_82.jpg&#39;),Path(&#39;images/scottish_terrier_2.jpg&#39;),Path(&#39;images/Russian_Blue_144.jpg&#39;),Path(&#39;images/pomeranian_166.jpg&#39;),Path(&#39;images/english_cocker_spaniel_48.jpg&#39;),Path(&#39;images/japanese_chin_180.jpg&#39;),Path(&#39;images/scottish_terrier_15.jpg&#39;),Path(&#39;images/Sphynx_166.jpg&#39;),Path(&#39;images/Maine_Coon_98.jpg&#39;)...] . . Define DataBlock . Let&#39;s construct a DataBlock object. A DataBlock object provides us encapsulation over many aspects of our data loading and arranging pipeline. It let&#39;s us define the . blocks which make up for X and y in our dataset . This will also automtically convert the labels to integer ids | . | extract the label from the name attribute of the file | apply Transformations for us which can help us do Data Augmentation and resizing in one go. | Randomly split the data into training and validation splits. | . Notice how it does all the work and more (we didn&#39;t do any augmentation) of the classes Splitter, RegexLabelExtractor, LabelManager, DatasetManager defined above in just one call, and since it&#39;s well maintained, offers us much more functionality, generic, more performamnt, well tested and maintained, we don&#39;t need to keep writing our own versions from scratch every time we are tasked with training a classifier. . pets = DataBlock(blocks = (ImageBlock, CategoryBlock), get_items=get_image_files, splitter=RandomSplitter(seed=42), get_y=using_attr(RegexLabeller(r&#39;(.+)_ d+.jpg$&#39;), &#39;name&#39;), item_tfms=Resize(460), batch_tfms=aug_transforms(size=224, min_scale=0.75)) . To check if everything will work fine, there&#39;s a pretty handy function called summary that we call on the DataBlock object that will show us the whole plan and will tell us a meningful error message if there&#39;s an issue with our pipeline somewhere. . Sanity Check . pets.summary(path/&quot;images&quot;) . Setting-up type transforms pipelines Collecting items from /root/.fastai/data/oxford-iiit-pet/images Found 7390 items 2 datasets of sizes 5912,1478 Setting up Pipeline: PILBase.create Setting up Pipeline: partial -&gt; Categorize -- {&#39;vocab&#39;: None, &#39;sort&#39;: True, &#39;add_na&#39;: False} Building one sample Pipeline: PILBase.create starting from /root/.fastai/data/oxford-iiit-pet/images/Persian_180.jpg applying PILBase.create gives PILImage mode=RGB size=334x500 Pipeline: partial -&gt; Categorize -- {&#39;vocab&#39;: None, &#39;sort&#39;: True, &#39;add_na&#39;: False} starting from /root/.fastai/data/oxford-iiit-pet/images/Persian_180.jpg applying partial gives Persian applying Categorize -- {&#39;vocab&#39;: None, &#39;sort&#39;: True, &#39;add_na&#39;: False} gives TensorCategory(7) Final sample: (PILImage mode=RGB size=334x500, TensorCategory(7)) Collecting items from /root/.fastai/data/oxford-iiit-pet/images Found 7390 items 2 datasets of sizes 5912,1478 Setting up Pipeline: PILBase.create Setting up Pipeline: partial -&gt; Categorize -- {&#39;vocab&#39;: None, &#39;sort&#39;: True, &#39;add_na&#39;: False} Setting up after_item: Pipeline: Resize -- {&#39;size&#39;: (460, 460), &#39;method&#39;: &#39;crop&#39;, &#39;pad_mode&#39;: &#39;reflection&#39;, &#39;resamples&#39;: (2, 0), &#39;p&#39;: 1.0} -&gt; ToTensor Setting up before_batch: Pipeline: Setting up after_batch: Pipeline: IntToFloatTensor -- {&#39;div&#39;: 255.0, &#39;div_mask&#39;: 1} -&gt; Flip -- {&#39;size&#39;: None, &#39;mode&#39;: &#39;bilinear&#39;, &#39;pad_mode&#39;: &#39;reflection&#39;, &#39;mode_mask&#39;: &#39;nearest&#39;, &#39;align_corners&#39;: True, &#39;p&#39;: 0.5} -&gt; RandomResizedCropGPU -- {&#39;size&#39;: (224, 224), &#39;min_scale&#39;: 0.75, &#39;ratio&#39;: (1, 1), &#39;mode&#39;: &#39;bilinear&#39;, &#39;valid_scale&#39;: 1.0, &#39;max_scale&#39;: 1.0, &#39;p&#39;: 1.0} -&gt; Brightness -- {&#39;max_lighting&#39;: 0.2, &#39;p&#39;: 1.0, &#39;draw&#39;: None, &#39;batch&#39;: False} Building one batch Applying item_tfms to the first sample: Pipeline: Resize -- {&#39;size&#39;: (460, 460), &#39;method&#39;: &#39;crop&#39;, &#39;pad_mode&#39;: &#39;reflection&#39;, &#39;resamples&#39;: (2, 0), &#39;p&#39;: 1.0} -&gt; ToTensor starting from (PILImage mode=RGB size=334x500, TensorCategory(7)) applying Resize -- {&#39;size&#39;: (460, 460), &#39;method&#39;: &#39;crop&#39;, &#39;pad_mode&#39;: &#39;reflection&#39;, &#39;resamples&#39;: (2, 0), &#39;p&#39;: 1.0} gives (PILImage mode=RGB size=460x460, TensorCategory(7)) applying ToTensor gives (TensorImage of size 3x460x460, TensorCategory(7)) Adding the next 3 samples No before_batch transform to apply Collating items in a batch Applying batch_tfms to the batch built Pipeline: IntToFloatTensor -- {&#39;div&#39;: 255.0, &#39;div_mask&#39;: 1} -&gt; Flip -- {&#39;size&#39;: None, &#39;mode&#39;: &#39;bilinear&#39;, &#39;pad_mode&#39;: &#39;reflection&#39;, &#39;mode_mask&#39;: &#39;nearest&#39;, &#39;align_corners&#39;: True, &#39;p&#39;: 0.5} -&gt; RandomResizedCropGPU -- {&#39;size&#39;: (224, 224), &#39;min_scale&#39;: 0.75, &#39;ratio&#39;: (1, 1), &#39;mode&#39;: &#39;bilinear&#39;, &#39;valid_scale&#39;: 1.0, &#39;max_scale&#39;: 1.0, &#39;p&#39;: 1.0} -&gt; Brightness -- {&#39;max_lighting&#39;: 0.2, &#39;p&#39;: 1.0, &#39;draw&#39;: None, &#39;batch&#39;: False} starting from (TensorImage of size 4x3x460x460, TensorCategory([ 7, 22, 6, 14], device=&#39;cuda:0&#39;)) applying IntToFloatTensor -- {&#39;div&#39;: 255.0, &#39;div_mask&#39;: 1} gives (TensorImage of size 4x3x460x460, TensorCategory([ 7, 22, 6, 14], device=&#39;cuda:0&#39;)) applying Flip -- {&#39;size&#39;: None, &#39;mode&#39;: &#39;bilinear&#39;, &#39;pad_mode&#39;: &#39;reflection&#39;, &#39;mode_mask&#39;: &#39;nearest&#39;, &#39;align_corners&#39;: True, &#39;p&#39;: 0.5} gives (TensorImage of size 4x3x460x460, TensorCategory([ 7, 22, 6, 14], device=&#39;cuda:0&#39;)) applying RandomResizedCropGPU -- {&#39;size&#39;: (224, 224), &#39;min_scale&#39;: 0.75, &#39;ratio&#39;: (1, 1), &#39;mode&#39;: &#39;bilinear&#39;, &#39;valid_scale&#39;: 1.0, &#39;max_scale&#39;: 1.0, &#39;p&#39;: 1.0} gives (TensorImage of size 4x3x224x224, TensorCategory([ 7, 22, 6, 14], device=&#39;cuda:0&#39;)) applying Brightness -- {&#39;max_lighting&#39;: 0.2, &#39;p&#39;: 1.0, &#39;draw&#39;: None, &#39;batch&#39;: False} gives (TensorImage of size 4x3x224x224, TensorCategory([ 7, 22, 6, 14], device=&#39;cuda:0&#39;)) . . Dataloader . Let&#39;s define our dataloader. . dls = pets.dataloaders(path/&quot;images&quot;) . Training (fine-tuning) the model . Let&#39;s train our model for two epochs using cnn_learner, the model we&#39;ll use is resnet34 . learn = cnn_learner(dls, resnet34, metrics=error_rate) learn.fine_tune(10) . epoch train_loss valid_loss error_rate time . 0 | 1.492460 | 0.365994 | 0.117727 | 01:09 | . epoch train_loss valid_loss error_rate time . 0 | 0.497617 | 0.326611 | 0.109608 | 01:14 | . 1 | 0.303163 | 0.237988 | 0.077131 | 01:14 | . Notice how we didn&#39;t have to worry about sending the data or the model over to the GPU. . Interpretation and Analysis . And the cherry on top is the ability to do interpretation and analyze errors with a very neatly written function call. . interp = ClassificationInterpretation.from_learner(learn) interp.plot_confusion_matrix(figsize=(12,12), dpi=60) . . Summary . And we&#39;re done! Sure the model can be improved upon from here, but the point is that I can now focus on that bit precisely after just getting started and not worry about anything else. I&#39;d advice now to please read the chapter 5 of the fastbook as the last few lines have missed a few points about Data Augmentation, finding the right Learning Rate etc. . References . Fastai docs | Fastbook Chapter 5 | Weights &amp; Biases forum for chapter 5 | Implementing Yann LeCun’s LeNet-5 in PyTorch | How can I disable all layers gradient expect the last layer in Pytorch? | Grayscale to RGB transform | PyTorch PIL to Tensor and vice versa | Deep residual Learning for Image Recognition | .",
            "url": "https://akashmehra.github.io/blog/classification/pytorch/fastai/2021/07/20/pets_classifier.html",
            "relUrl": "/classification/pytorch/fastai/2021/07/20/pets_classifier.html",
            "date": " • Jul 20, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "I am a Machine Learning Engineer. This is my personal blog1 where I write about my learnings in the field of Machine Learning and Artifical Intelligence. I am very passionate about math and its applications and usually find myself reading math theory in my free time. I love playing football and cricket when I am not working, reading, sleeping or working out. . [1] The views expressed here are mine alone and not those of my employer. .",
          "url": "https://akashmehra.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://akashmehra.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}