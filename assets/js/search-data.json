{
  
    
        "post0": {
            "title": "Loss Functions Part 2",
            "content": "import torch from torch import nn from torch.autograd import Variable import numpy as np precision=4 np.set_printoptions(precision=precision) . . In this post, I&#39;d like to ensure that we&#39;re able to code the loss classes ourselves as well, that means that we&#39;ll need to be able to: . Translate the equations to Python code for forward pass. | Know how to calculate the gradients for a few loss functions, so that when we call backward pass the gradients get accumulated. | Note that in presence of autodiff or autograd we won&#39;t need to do such a thing and this is purely for learning perspective, better implementations are offered by Deep Learning Libraries like PyTorch, Tensorflow etc. . So, we&#39;ll define a class called Module that we&#39;ll use and override its forward and backward abstract functions . from abc import ABC, abstractmethod class Module(ABC): def __init__(self, reduction=&#39;mean&#39;): self.grad_input = None self.reduction = reduction def __call__(self, input, target): return self.forward(input, target) @abstractmethod def forward(*args, **kwargs): pass @abstractmethod def backward(*args, **kwargs): pass def update_grad_input(self, grad): if self.grad_input is not None: self.grad_input += grad else: self.grad_input = grad def zero_grad(self): if self.grad_input is not None: self.grad_input = np.zeros_like(self.grad_input) . After having setup our base class, we can now look at individual Loss functions and start coding them. . Mean Squared Error . Mean Squared Error (MSE) measures the average of the squares of the errors for an estimator. The MSE of an estimator $ hat{ theta}$​ wrt an unknown parameter $ theta$​ is defined as $MSE( hat{ theta}) =E[( hat{ theta} - theta)^2]$​​​. It can also be written as the sum of Variance and the squared Bias of the estimator. In case of an unbiased estimator the MSE and variance are equivalent. Below is a quick way to see how the MSE and the Variance and Bias are related to each other. . $$ E[X] = mu $$ $$ Var(X) = E[(X - mu)^2] = E[X^2] + mu^2 - 2 mu E[X] implies E[X^2] - E[X]^2 $$ using the following result: . $$ Var(X-Y) = Var(X) + Var(Y) - 2Cov(X,Y) $$ $$ Var( mu) = 0 $$ $$ Cov(X, mu) = E[X-E[X]]E[ mu-E[ mu]] = 0 implies Var(X- mu) = Var(X) $$ We arrive at: . $$ MSE( hat{ theta}) = E[( hat{ theta} - theta)^2] implies Var( hat{ theta} - theta) + E[ hat{ theta} - theta]^2 $$ $$ MSE( hat{ theta}) = Var[ hat{ theta}] + Bias^2[ hat{ theta}] $$ We can see from above that as Variance, MSE also heavily weights the outliers. It&#39;s squaring each term which weights large error terms more heavily than smaller ones. An alternative in that becomes Mean Absolute Error which is the topic of discussion of next section. . MSE Loss in numpy . We&#39;re using our implementation of the Module class. We&#39;ll define forward and backward functions for MSE Loss . class MSELoss(Module): def __init__(self, reduction=&#39;mean&#39;): super(MSELoss, self).__init__(reduction) def forward(self, input, target): red = getattr(np, self.reduction) return np.asarray([red((target-input)**2),]) def backward(self, input, target): N = np.multiply.reduce(input.shape) grad = (2/N * (input-target)) self.update_grad_input(grad) . reduction = &#39;mean&#39; SEED=42 rs = np.random.RandomState(SEED) input = rs.randn(3,2,4,5) target = rs.randn(3,2,4,5) . mse = MSELoss(reduction) loss = mse(input, target) loss . array([1.7061]) . mse.backward(input, target) . Let&#39;s test our implementation against PyTorch and see if we&#39;ve implemented things correctly or not. . MSE Loss in PyTorch . Using the same input and target numpy.ndarray as torch.tensor . criterion = nn.MSELoss(reduction=reduction) inp = Variable(torch.from_numpy(input), requires_grad=True) loss = criterion(inp, torch.from_numpy(target)) loss.backward() . np.allclose(mse.grad_input, inp.grad.numpy()) . True . Mean Absolute Error Loss (MAE) . As pointed out earlier the MSE Loss suffers in the presence of outliers and heavily weights them. MAE on the other hand is more robust in that scenario. It is defined as the average of the absolute differences between the predictions and the target values. . $$ MAE( hat{ theta}) = (E[| hat{ theta} - theta|]) $$It is also knows as the L1Loss as it is measuring the L1 distance between two vectors . MAE/L1 Loss in numpy . We&#39;ll use our implementation of the Module here as well and define the forward and backward function for L1Loss . class L1Loss(Module): def __init__(self, reduction=&#39;mean&#39;): super(L1Loss, self).__init__(reduction) def forward(self, input, target): red = getattr(np, self.reduction) return np.asarray([red((np.abs(target-input))),]) def backward(self, input, target): N = np.multiply.reduce(input.shape) diff = input - target mask_lg = (diff &gt; 0) * 1.0 mask_sm = (diff &lt; 0) * -1.0 mask_zero = (diff == 0 ) * 0.0 grad = 1/N * (mask_lg + mask_sm + mask_zero) self.update_grad_input(grad) . reduction = &#39;mean&#39; SEED=42 rs = np.random.RandomState(SEED) input = rs.randn(3,2,4,5) target = rs.randn(3,2,4,5) . l1loss = L1Loss(reduction) l1loss(input, target) . array([1.0519]) . l1loss.backward(input, target) . As before, let&#39;s test our implementation against PyTorch and see if we&#39;ve implemented things correctly or not. . MAE/L1 Loss in PyTorch . criterion = nn.L1Loss(reduction=&#39;mean&#39;) inp = Variable(torch.from_numpy(input), requires_grad=True) loss = criterion(inp, torch.from_numpy(target)) loss . tensor(1.0519, dtype=torch.float64, grad_fn=&lt;L1LossBackward&gt;) . loss.backward() . np.allclose(l1loss.grad_input, inp.grad) . True .",
            "url": "https://akashmehra.github.io/blog/loss_functions/2021/08/10/loss_functions_part2.html",
            "relUrl": "/loss_functions/2021/08/10/loss_functions_part2.html",
            "date": " • Aug 10, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Loss Functions Part 1",
            "content": "Introduction . A Loss/Cost function or an objective function in Machine Learning (ML) is a function that maps the values of one or more variables to a scalar value. This scalar value is then used to minimize the loss/cost incurred while making a prediction. On a very high level, an ML algorithm works as follows: . . The diagram above is a very high-level overview of a model learning from data. We&#39;ve simplified the training workflow a lot in the above diagram, we also have a step where the gradient of the loss wrt the model parameters gets calculated (we&#39;ll talk about autodiff in a detailed post later on) and then the optimizer uses those acumulated gradients for each parameter and updates the parameters for next iteration. . The choice of a loss function plays an important role in learning from data. It depends on what we want to learn, and what we want to predict. In some cases we just want to predict the distribution of the categories (discrete values), there we end up using a certain type of loss function while in some other cases we&#39;d like to predict continuous values and there the choice of loss function would be completely different. Some time we&#39;d like to perform metric learning, and the loss function for such a problem would not fall in the aforementioned categories. Below we review some of the loss functions that are most commonly used for some of the common problems in the field of ML. . Types of Loss Functions . Classification . Cross Entropy Loss . Entropy . There&#39;s something to be said about Entropy before we look into Cross-Entropy. Entropy, is such a beautiful concept that I got first introduced to in an information theory course I took years ago. The whole process can be seen as how many bits we need to encode the message on the transmitter side to be able to successfully decode at the receiver. The concept was introduced by Claude Shannon and it is also known as Shannon entropy. . . Source: A Mathematical Theory of Communication Formally, given a discrete random variable $X$, with possible outcomes $x_1,x_2,...,x_n$, which occur with probability $P(x_1),P(x_2),...,P(x_n)$ the entropy of $X$​ is given by: $$ boxed{H(X) = - sum_{i=1}^{n} P(x_i) log(P(x_i))} $$ What follows next is the characterization and can be found here on Wikipedia) . To understand the meaning of the above equation, first define an information function $I$ in terms of an event $x_i$ with probability $P(x_i)$. The amount of information acquired due to the observation of event $x_i$ follows from Shannon&#39;s solution of the fundamental properties of information. . $I(p)$ is monotonically decreasing in $p$, an increase in the probability $p$ of an event decreases the information observed. | $I(p) geq 0$, information is a non-negative quantity | $I(1) = 0$, events that always occur, don&#39;t convey any information | $I(p_1, p_2)$ = $I(p_1) + I(p_2)$ if $p_1$ and $p_2$ are independent. | Shannon discovered that the suitable choice for $I$ is given by $I(p) = log(p)$​. . Using above to analyze the equation of $H(X)$​​​​​​​, we see that the number of bits required to encode a message when $P(x_i) = 1$​​​​​​ or $P(x_i) = 0$​​​​​​ is $0$​​​​​​, this is due to the fact that we&#39;re either sure that the event will happen or it won&#39;t (we&#39;ll not communicate). An increase in the value of $P(x_i)$​​​​​ decreases the value of the measure (Entropy). . Below is the plot of Information vs Probability or $I(p)$​​ vs $p$ which tells us that Information​ conveyed by the event decreases as Probability increases . from matplotlib import pyplot as plt import numpy as np import pandas as pd from scipy.stats import norm precision = 4 np.set_printoptions(precision=precision) pd.set_option(&quot;precision&quot;, precision) import torch from torch.nn import functional as F class InformationMeasure(): @staticmethod def entropy(prob): return np.sum(-prob * np.log2(prob, out=np.zeros_like(prob), where=(prob!=0))) class Plotter(): @staticmethod def plot_xy(x,y, x_label, y_label, title, show_grid=True, legend=True, legend_loc=&#39;upper right&#39;, x_ticks=np.arange(0.0, 1.1, 0.5), plot_point=False, show_axes=True): with plt.xkcd(): #plt.style.use(&#39;ggplot&#39;) fig, ax = plt.subplots(figsize=(8, 5)) fig.tight_layout() ax.plot(x,y, linewidth=2.0, color=&#39;brown&#39;) if plot_point: ax.plot(0.5, 1.0, &#39;go&#39;, label=&#39;P(X = 1) = 0.5&#39;) ax.set_xlabel(x_label, fontsize=10, color=&#39;black&#39;) ax.set_ylabel(y_label, fontsize=10, color=&#39;black&#39;) ax.set_title(title, fontsize=15, color=&#39;black&#39;) ax.grid(False) if not show_axes: plt.axis(&#39;off&#39;) if legend: ax.legend(loc=legend_loc) x_ticks = ax.set_xticks(x_ticks) return fig, ax class CoinToss(): def __init__(self): self.x = np.arange(0.0, 1.0, 0.01) self.x = np.append(self.x, 1.0) self.hx = np.asarray([np.asarray(InformationMeasure.entropy(np.asarray(prob))) for prob in zip(self.x, 1-self.x)], dtype=np.float64) def plot(self): return Plotter.plot_xy(self.x, self.hx, x_label=&#39;P(X = 1)&#39;, y_label=&#39;H(X)&#39;, title=&#39;Entropy of a coin flip&#39;, plot_point=True) class EntropyFunction(): def __init__(self): self.x = np.arange(0.1, 1.0, 0.01) self.x = np.append(self.x, 1.0) self.hx = -np.log2(self.x, out=np.zeros_like(self.x), where=(self.x!=0)) def plot(self): return Plotter.plot_xy(self.x, self.hx, x_label=&#39;$P(X)$&#39;, y_label=&#39;$I(P(X))$&#39;, title=&#39;Information vs Probability&#39;, legend=False, x_ticks=np.arange(0.1, 1.1, 0.1)) entropy_func = EntropyFunction() fig, ax = entropy_func.plot() . . Now, let&#39;s consider as an example, tossing a biased coin whose probability of landing on heads is $p$​ and the probability of landing on tails is $1-p$​​, this can be modelled as a Bernoulli Process. Below is the Entropy vs Probability plot for the process: . fig, ax = CoinToss().plot() . . Cross-Entropy . Building upon the concept of Entropy we come across Cross-Entropy which tells us the average number of bits required to encode the information coming from a source distribution $P$ using a model distribution $Q$. $$ H(P, Q) = - sum_{i=1}^{n} P(x_i) log (Q(x_i)) $$ Here, we generally don&#39;t have access to the target distribution $P$​​, so we approximate it by the model distribution $Q$​​, and the closer $Q$​ is to $P$​ the lesser the average number of bits required to encode the information. One could also write $H(P,Q)$ in terms of expectation as follows: $$ H(P, Q) = - mathbb{E}_{P(x)}[ log(Q(x))] $$ . KL-Divergence . KL-Divergence (also known as relative entropy) is a type of f-divergence, which is a measure of how one probability distribution is different from the other. In Information theory terms, it&#39;s the extra number of bits required to encode the information from source distribution $P$​​​​ when using the model distribution $Q$​​​. The divergence of $Q$​​ from $P$​ is defined as: $$ mathbb{D}_{KL}(P || Q) = sum_{i=1}^{n} P(x_i) log ( frac {Q(x_i)}{P(x_i)}) $$ . Even though KL divergence is used to measure the distance between two distributions but it&#39;s a ditance metric. Some properties of KL Divergence are: . It&#39;s not symmetric, i.e $ mathbb{D}_{KL}(P || Q) != mathbb{D}_{KL}(Q || P)$ | It&#39;s non-negative, i.e $ mathbb{D}_{KL}(P || Q) &gt;= 0$ and $ mathbb{D}_{KL}(P || Q) = 0$ only if $P = Q$ | . Some intuitions and explanations about KL Divergence: . If $P(x) = 0$ then during optimization it doesn&#39;t matter what the value of $Q(x)$ is as KL evaluates to 0. What this means is that when we don&#39;t have any weight in the source distribution but we do in target distribution, the target distribution is ignored. . | If $P(x) &gt; 0$, then we have a divergence, and if the modes of $Q(x)$ and $P(x)$ don&#39;t overlap then we&#39;ll incur a cost. The optimizer will then try to spread $Q(x)$ out so that the divergence is minimized. . | . Let&#39;s look at the graph below to make sense of what has been said in the two points above: . The figure above demonstrates the case where $P(x) &gt; 0$ and $Q(x)$ doesn&#39;t match all the modes of the distribution, in this case KL Divergence will be high and the optimizer will try to move it $Q(x)$ such that it covers parts pf both the modes as shown below: . For this reason, this form of KL Divergence is known as zero-avoiding as it is avoiding $Q(x) = 0$ whenever $P(x) &gt; 0$ . There&#39;s one more form of KL Divergence as well, known as Reverse KL Divergence, that&#39;s something we&#39;ll cover when we talk about Variational Inference, the reason for that is that we need to talk about ELBO (Evidence Lower Bound) . Putting it all together . One can see that there exists a relationship between $ mathbb{D}_{KL}(P || Q)$​ and $H(P,Q)$. Let&#39;s see if we can write one in terms of the other. $$ mathbb{D}_{KL}(P || Q) = sum_{i=1}^{n} P(x_i) log ( frac {Q(x_i)}{P(x_i)}) implies - mathbb{E}_{P(x)} left [log left( frac{Q(x)}{P(x)} right) right] implies - mathbb{E}_{P(x)}[ log(Q(x))] + mathbb{E}_{P(x)}[ log(P(x))] $$ . $$ mathbb{D}_{KL}(P || Q) = - mathbb{E}_{P(x)}[ log(Q(x))] + mathbb{E}_{P(x)}[ log(P(x))] $$$$ mathbb{D}_{KL}(P || Q) = H(P,Q) - H(P) implies H(P,Q) = H(P) + mathbb{D}_{KL}(P || Q) $$Calculating Cross Entropy and KL Divergence . The code for calculating Cross Entropy and KL Divergence is just the direct translation of the equations above. . In what follows next, we&#39;re assuming that the input to these functions are probability distributions. . def cross_entropy(p, q): # both p and q are asuumed be probability distributions. return np.sum(-p * np.log(q)) . A way to turn scores into probability distribution is via Softmax function, which is defined as follows: . def softmax(p): return np.exp(p)/np.sum(np.exp(p), axis=-1, keepdims=True) . Notice how we&#39;re using the axis parameter to only take the sum along the last dimension in case we&#39;re dealing with more than one dimensions (which usually is the case). . Next, we&#39;ll create sample probability distributions for demonstration purposes. . def get_sample_dist(rs, size=(10,)): # Return a sample (or samples) from the &quot;standard normal&quot; distribution. scores = rs.randn(*size) # covert to a probability distribution p = softmax(scores) return scores, p . def sanity_check_dist(p): if p.ndim &gt; 1: expected = np.ones(p.shape[-2]) else: expected = 1.0 return np.allclose(np.sum(p, axis=-1), expected) . . def print_as_table(p): return pd.DataFrame(data=p, index=[f&quot;{i}&quot; for i in range(p.shape[0])], columns=[f&quot;{i}&quot; for i in range(p.shape[1])]) . . We&#39;ll now out to test what we&#39;ve coded so far . seed = 42 rs = np.random.RandomState(seed) scores_p, p = get_sample_dist(rs, (10,5)) scores_q, q = get_sample_dist(rs, (10,5)) sanity_check_dist(p), sanity_check_dist(q) . Here&#39;s how $p^*$ looks like: . $^*$For demonstration purposes, I&#39;ve chosen the distributions to be 2 dimensional so that it can be visually inspected. . print_as_table(p) . . 0 1 2 3 4 . 0 0.1676 | 0.0888 | 0.1950 | 0.4678 | 0.0807 | . 1 0.0780 | 0.4783 | 0.2124 | 0.0617 | 0.1696 | . 2 0.2203 | 0.2197 | 0.4459 | 0.0517 | 0.0624 | . 3 0.1932 | 0.1231 | 0.4643 | 0.1368 | 0.0826 | . 4 0.6170 | 0.1137 | 0.1524 | 0.0343 | 0.0827 | . 5 0.2670 | 0.0756 | 0.3479 | 0.1311 | 0.1785 | . 6 0.0520 | 0.6052 | 0.0937 | 0.0330 | 0.2161 | . 7 0.0936 | 0.3911 | 0.0447 | 0.0841 | 0.3864 | . 8 0.4073 | 0.2310 | 0.1734 | 0.1440 | 0.0444 | . 9 0.0873 | 0.1131 | 0.5160 | 0.2528 | 0.0308 | . Here&#39;s how $q$ looks like: . print_as_table(q) . . 0 1 2 3 4 . 0 0.1916 | 0.0943 | 0.0704 | 0.2554 | 0.3884 | . 1 0.3275 | 0.0558 | 0.0947 | 0.1797 | 0.3423 | . 2 0.1428 | 0.1915 | 0.0763 | 0.0697 | 0.5197 | . 3 0.4086 | 0.0979 | 0.2871 | 0.1511 | 0.0552 | . 4 0.1205 | 0.3909 | 0.0810 | 0.4014 | 0.0061 | . 5 0.4260 | 0.2043 | 0.1389 | 0.2052 | 0.0257 | . 6 0.1048 | 0.1867 | 0.5725 | 0.0778 | 0.0582 | . 7 0.0897 | 0.3699 | 0.2058 | 0.0872 | 0.2474 | . 8 0.1958 | 0.4681 | 0.0881 | 0.1280 | 0.1200 | . 9 0.0496 | 0.2879 | 0.2780 | 0.2152 | 0.1693 | . print(f&#39;Cross Entropy between P and Q: {cross_entropy(p,q): .4f}&#39;) print(f&#39;Cross Entropy between P and P: {cross_entropy(p,p): .4f}&#39;) print(f&#39;Entropy of P: {entropy(p): .4f}&#39;) . Cross Entropy between P and Q: 18.2452 Cross Entropy between P and P: 13.2577 Entropy of P: 13.2577 . The code snippet calculates cross entropy, to calculate kl divergence we&#39;ll introduce another function . def kl_div(p, q): return np.sum(-p * (np.log(q) - np.log(p))) print(f&#39;KL Divergence of Q from P: {kl_div(p,q): .4f}&#39;) . KL Divergence of Q from P: 4.9875 . To verify the relationship between Cross Entropy and Kl Divergence let&#39;s introduce another function to calculate entropy: . def entropy(p): return np.sum(-p * np.log(p)) np.allclose(kl_div(p,q) + entropy(p), cross_entropy(p,q)) . True . Now, let&#39;s take a look at how the Cross Entropy Loss is implemented in a Machine Learning framework like PyTorch, from the PyTorch Documnetation about Cross Entropy Loss (also known as categorical cross entropy) . &quot;This criterion combines LogSoftmax and NLLLoss in one single class. It is useful when training a classification problem with C classes.&quot; . So, it turns out, we only need to send in scores to this function as it combines LogSoftMax and NLLLoss. We also need to send in class labels to the function, effectively the function caluclates the following: . $$ loss(x, class) = - log left[ frac{ exp(x[class])}{ sum_{j} x[j]} right] implies -x[class] + log left[ sum_{j} exp(x[j]) right] $$ torch.manual_seed(42) minibatch_size = 10 num_classes = 5 inputs = torch.randn(minibatch_size, num_classes) targets = torch.empty(minibatch_size, dtype=torch.long).random_(num_classes) loss = F.cross_entropy(inputs, targets) loss . tensor(2.2212) . Let&#39;s use our inputs from before, remember that we have to send raw-scores and class labels. targets correspond to p and inputs correspond to scores_q . inputs = torch.from_numpy(scores_q) targets = torch.from_numpy(np.argmax(p, axis=1)) . loss = F.cross_entropy(inputs, targets) loss . tensor(1.7747, dtype=torch.float64) . Notice how F.cross_entropy calculates the cross entropy between class labels and a probability distribution, that&#39;s the only difference (conceptually speaking) between the code we&#39;ve written to calculate the the cross entropy and the PyTorch version . def cross_entropy_loss(inputs, targets): return np.asarray([np.mean(-np.take_along_axis(inputs, targets, 1) + np.log(np.sum(np.exp(inputs), axis=-1, keepdims=True)))]) . There&#39;s another way to implement this function using for loops, but numpy broadcasting is much faster and we should always use that when we can . classes = np.expand_dims(np.argmax(p, axis=1), axis=1) cross_entropy_loss(scores_q, classes) . array([1.7747]) . Summary . We looked at the concepts like Entropy, Cross Entropy and KL Divergence from the point of view of Information Theory and got a sense of how to implement them and how they&#39;re implemented in a Deep Learning Framework like PyTorch, from here things should get a lot easier when we think about Cross Entropy and KL Divergence . I&#39;ll stop here for now and in the next post in the series about Loss Functions cover the following: . MSE | MAE | Huber Loss | Triplet Loss | Hinge Loss | References . A Mathematical Theory of Communication | Cross Entropy Loss VS Log Loss VS Sum of Log Loss | A Gentle Introduction to Cross-Entropy for Machine Learning | nn.CrossEntropyLoss | Lecture 2: Entropy and Data Compression (I): Introduction to Compression, Inf.Theory and Entropy, David MacKay, University of Cambridge | Variational Inference | KL Divergence Forward vs Reverse | Overlapping probability of two normal distribution with scipy | Notes on KL Divergence | PyTorch Loss Functions: Ultimate Guide | .",
            "url": "https://akashmehra.github.io/blog/loss_functions/2021/08/04/loss_functions_part1.html",
            "relUrl": "/loss_functions/2021/08/04/loss_functions_part1.html",
            "date": " • Aug 4, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Optimization  Notes",
            "content": "import numpy as np from mpl_toolkits.mplot3d import Axes3D import matplotlib.pyplot as plt fig = plt.figure(figsize=(10,8)) ax = fig.add_subplot(111, projection=&#39;3d&#39;) plot_args = {&#39;rstride&#39;: 1, &#39;cstride&#39;: 1, &#39;cmap&#39;:&quot;Blues_r&quot;, &#39;linewidth&#39;: 0.4, &#39;antialiased&#39;: True, &#39;vmin&#39;: -1, &#39;vmax&#39;: 1} x, y = np.mgrid[-1:1:31j, -1:1:31j] z = x**2 - y**2 ax.plot_surface(x, y, z, **plot_args) ax.plot([0], [0], [0], &#39;ro&#39;) ax.grid() plt.axis(&#39;off&#39;) ax.set_title(&quot;Saddle Point&quot;) plt.show() . . Image Credit: By Nicoguaro - Own work, CC BY 3.0, https://commons.wikimedia.org/w/index.php?curid=20570051 Optimization basics . Univariate Optimality Conditions: Quick recap . Consider a function $f(x)$​, where $x$​ is univariate, the necessary conditions for a point $x=x_0$​ to be a minimum of $f(x)$​ with respect to its infinitesimal locality are: $f&#39;(x_0) = 0$​ and $f&#39;&#39;(x_0) &gt; 0$​. The optimality conditions can be well undertsood if we look at the Taylor Series expansion of $f(x)$​ in the small vicinty of $x_0 + Delta$​. $$ f(x_0 + Delta) approx f(x_0) + Delta f&#39;(x) + frac{ Delta^2}{2} f&#39;&#39;(x) $$ Here, the value of $ Delta$​ is assumed to be very small. One can see that if $f&#39;(x) = 0$​ and $f&#39;&#39;(x) &gt; 0$​ then $f(x_0 + Delta) approx f(x_0) + epsilon$​​, which means that $f(x_0) &lt; f(x_0 + Delta)$​, for small values of $ Delta$​ (whether it is positive or negative) or $x_0$​ is a minimum wrt to its immediate locality. . Multivariate Optimality Conditions . Consider a function $f(x)$ where $x$ is an n-dimensional vector given by $ begin {bmatrix}x_1, x_2, x_3, cdots, x_n end {bmatrix}^T$. The gradient vector of $f(x)$ is . given by the partial derivatives wrt to each of the components of $x$, $ nabla {f{x}} equiv g(x) equiv begin {bmatrix} frac{ partial f}{ partial x_1} frac{ partial f}{ partial x_2} frac{ partial f}{ partial x_3} vdots frac{ partial f}{ partial x_n} end{bmatrix}$ . Note that the gradient in case of a multivariate functions is vector of n-dimensions. Similarly, one can define the second derivative of a multivariate function using a matrix of size $n times n$​ $$ nabla^2f(x) equiv H(x) equiv begin{bmatrix} frac{ partial^2 f}{ partial x_{1}^2} &amp; cdots &amp; frac{ partial^2 f}{ partial x_{1} partial x_{n}} vdots &amp; ddots &amp; vdots frac{ partial^2 f}{ partial x_{n} partial x_{1}} &amp; cdots &amp; frac{ partial^2 f}{ partial x_{n}^2} end{bmatrix} $$ Here, $H(x)$​​​​ is called a Hessian Matrix, and if the partial derivatives ${ partial^2 f}/{ partial x_{i} partial x_{j}}$​​​​​ and ${ partial^2 f}/{ partial x_{j} partial x_{i}}$​​​​​ are both defined and continuous and then by Clairaut&#39;s Theorem $ partial^2 f/ partial x_{i} partial x_{j}$​​​​​​​ = $ partial^2 f/ partial x_{j} partial x_{i}$​​​​​,​​ this second order partial derivative matrix becomes symmetric. . If $f$​ is quadratic then the Hessian becomes a constant, the function can then be expressed as: $f(x) = frac{1}{2}x^THx + g^Tx + alpha$​ , and as in case of univariate case the optimality conditions can be derived by looking at the Taylor Series expansion of $f$​ about $x_0$: $$ f(x_0 + epsilon overline{v}) = f(x_0) + epsilon overline{v}^T nabla f(x_0) + frac { epsilon^2}{2} overline{v}^T H(x_0 + epsilon theta overline{v}) overline{v} $$ where $0 leq theta leq 1$​, $ epsilon$​ is a scalar and $ overline{v}$​ is an n-dimensional vector. Now, if $ nabla f(x_0) = 0$​, then it leaves us with $f(x_0) + frac{ epsilon^2}{2} overline{v}^T H overline{v}$​, which implies that for the $x_0$​ to be a point of minima, $ overline{v}^T H overline{v}&gt; 0$​​ or the Hessian has to be positive definite. . Quick note on definiteness of the symmetric Hessian Matrix . $H$ is positive definite if $ mathbf{v}^TH mathbf{v} &gt; 0$, for all non-zero vectors $ mathbf{v} in mathbb{R}^n$ (all eigenvalues of $H$ are strictly positive) | $H$ is positive semi-definite if $ mathbf{v}^TH mathbf{v} geq 0$, for all non-zero vectors $ mathbf{v} in mathbb{R}^n$ (eigenvalues of $H$ are positive or zero) | $H$ is indefinite if there exists a $ mathbf{v}, mathbf{u} in mathbb{R}^n$, such that $ mathbf{v}^TH mathbf{v} &gt; 0$ and $ mathbf{u}^T H mathbf{u} &lt; 0$ (eigenvalues of $H$ have mixed sign) | $H$ is negative definite if $ mathbf{v}^TH mathbf{v} &lt; 0$, for all non-zero vectors $ mathbf{v} in mathbb{R}^n$ (all eigenvalues of $H$ are strictly negative) | . Gradient based optimization . Gradient based optimization is a technique to minimize/maximize the function by updating the paremeters/weights of a model using the gradients of the Loss wrt to the parameters. If the Loss function is denoted by $E(w)$​​​​​​​​, where $w$​​​​​​​​ are the parameters then we&#39;d like to calculate $ nabla_{w} E( mathbb{w})$​​​​​​​​ and to get the parameters for the next iteration, we&#39;d like to perform the $ mathbb{w}_{t+1} = mathbb{w}_{t} - eta * nabla_{ mathbb{w}} E( mathbb{w}_{t})$​​​​​, where $ eta$​​​​​​​​​ is the learning rate. . In order to update the parameters of a Machine Learning model we need a way for us to measure the rate of change in the output when the inputs (the parameters) are changed. . Here, we&#39;ve assumed that the function $E( mathbb{w})$​​​​ is continuous. The function $E( mathbb{w})$​​​ can have kinks in which case we&#39;ll call the gradient a subgradient. Subgradient generalizes the notion of a derivative to functions that are not necessirily differentiable. More on subgradients will follow in a separate post, but for now assume that there exists a concept using which one can calculate the gradient of a function that is not differentiable everywhere (has kinks, for example:ReLU non-linearity). There are many gradient based optimization algorithms that exist and they differ mainly in how the gradients are calculated or how the learning rate $ eta$​​ is chosen. We&#39;ll look at some of those algorithms that are used in practice. . Need for Gradient Descent . One of the necessary conditions for a point to be a critical point (minima, maxima or saddle) is that the first order derivate $f&#39;(x) = 0$​, it is often the case that we&#39;re not able to exactly solve this equation because the derivative can be a complex function of $x$​. A closed form solution so to speak, doesn&#39;t exist and things get even more complicated in multivariate case due to compultational and numerical challenges[1]. We use Gradient Descent to iteratively solve the optimization problem irrespective of the functional form of $f(x$​) by taking a step in the direction of the steepest descent (because in Machine Learning we&#39;re optimizing a Loss or a Cost function, we tend to always solve the optimization problem from the perspective of minimization). . Convergence of Gradient Descent . We also need to talk about the convergence of Gradient Descent before we can dive into different types of gradient based methods. Let&#39;s look at the update equation once again: $ mathbb{w}_{t+1} = mathbb{w}_{t} - eta frac{ partial E( mathbb{w})}{ partial mathbb{w}}$​​​​​​, to see the effect of learning rate in univariate case let&#39;s take a look athe following figures: . . Source: Efficient Backprop, Lecun et al. 98 $ eta_{opt}$​​​ is the optimal learning rate, and we can see in a) if our chosen learning rate $ eta &lt; eta_{opt}$​​​ then converges will happen at a slower pace, in (b we see that when $ eta = eta_{opt}$​​​ then we just converge right away and for $ eta_{opt} &lt; eta &lt; 2 eta_{opt}$​​​ the weights oscilate around the minimum but evenbtually converge. Things get difficult in case where $ eta &gt; 2 eta_{opt}$​​​​, when this happens, weights diverge. We also need to find out the value of $ eta_{opt}$​​​, and to do so we need to write out the Taylor Series expansion of our function $E( mathbb{w})$​​​ about current weight $ mathbb{w}_{c}$​. As we know from Section 1.2​, we could expand our function as: $$ E( mathbb{w}) = E( mathbb{w}_{c}) - ( mathbb{w} - mathbb{w}_{c}) frac { partial E( mathbb{w})}{ partial mathbb{w}} + frac{1}{2} ( mathbb{w} - mathbb{w}_{c})^2 frac { partial^2 E( mathbb{w})}{ partial mathbb{w}^2} + cdots, $$ as before, if $E( mathbb{w})$​ is quadratic then we&#39;re left with only the first and second order terms. Differentiating both sides wrt w and noting that higher order terms will vanish as the second order derivative itself is a constant, we&#39;re left with: $$ frac { partial E( mathbb{w})}{ partial mathbb{w}} = frac { partial E( mathbb{w_{c}})}{ partial mathbb{w}} + ( mathbb{w} - mathbb{w}_{c}) frac { partial^2 E( mathbb{w})}{ partial mathbb{w}^2} $$ Now setting $ mathbb{w} = mathbb{w}_{min}$​​ and noting that $ frac { partial E( mathbb{w}_{min})}{ partial mathbb{w}} = 0$, we get $$ ( mathbb{w}_{c} - mathbb{w}_{min}) frac { partial^2 E( mathbb{w})}{ partial mathbb{w}^2} = frac { partial E( mathbb{w_{c}})}{ partial mathbb{w}} implies boxed { mathbb{w}_{min} = mathbb{w}_{c} - left( frac { partial^2 E( mathbb{w})}{ partial mathbb{w}^2} right)^{-1} frac { partial E( mathbb{w_{c}})}{ partial mathbb{w}}} $$ The boxed equation looks a lot familiar, it turns out that it&#39;s our weight update equation which tells us that we can reach the minimum in one step if we set $ eta_{opt} = left( frac { partial^2 E( mathbb{w})}{ partial mathbb{w}^2} right)^{-1} $​, one extending this multivariate case we get $ eta_{opt} = H^{-1}( mathbb{w})$​​. . This takes us to the Newton based methods (a type of gradient based optimization). Note how we don&#39;t have to rey on a hyperparameter like the learning rate if we could somehow compute the inverse of the Hessian matrix. Multiplying the gradient vector with the inverse of the Hessian takes smaller steps in the direction of steep curvatire but takes larger steps in the direction of shallow curvature. Although, in theory it sounds nice but it&#39;s often very hard to compute the inverse of the Hessian for most practical situations. Algorithms like L-BFGS exist that reduce the memory requirements needed for Newton based methods but in practice we hardly see them applied in training Deep Neural Networks. . For more on the convergence theory, please read Section 5 of Efficient Backprop by Yann LeCun. . Gradient Descent and its variants . in this section I&#39;ll cover a few common variants of Gradient Descent that are most commonly used in practice, a more comprehensive treatment on the topic is convered in [2] and it&#39;s an amazing text to refer. . Stochastic Gradient Descent (SGD) . The variant of Gradient Descent that we&#39;ve seen so far is commonly known as Batched Gradient Descent, which requires us to calculate the gradients at each step by considering the whole dataset. As an alternate one can just use one example chosen at random from the whole dataset to calculate the gradient as well. This gradient albiet noisy, leads to better solutions. In code it looks something like this . for epoch in range(num_epochs): np.random.shuffle(data) for x,y in data: grads = eval_grads(loss, params, x, y) params -= learning_rate * grads . Here, the tuple x,y is one example, label pair samped from the dataset. Note that as previously mentioned, the updates performed using one example are very noisy. A graph of SGD fluctuation is follows: . Image Credit: Wikipedia With Batched Gradient Descent, it converges to the minima of the basin where the weights were initialized, whereas with Stochastic Gradient Descent, due to the noisy nature of the updates, the weights can potentially jump out of the basin and find a better minima. One thing to note is that this noise can make the convergence a lot slower as well as the weights can keep overshooting the minima, but it has been shown that on slowly annealing the learning rate, one can converge to a minima. Please see [1] . Mini-batch Gradient Descent . Mini Batch Gradient Descent is a good middle ground between the more expensive Batched version and the noisier Stochastic version of Gradient Descent. In this version, instead of sampling one exa,mple at random at a time, we sample a mini batch of a pre-defined size (which is a hyper-parameter). The benefit of this method is that it can help reduce the variance of the parameter updates (there by making the updates less noisy and having less oscialltions), it can also help us leverage the state of the art Deep Learning software libraries that have an efficient way of calculating radients for mini batches . Again, in code it looks something like this: . for epoch in range(num_epochs): batch_generator = BatchGenerator(batch_size=64) for batch in batch_generator: grads = eval_grads(loss, params, batch) params -= learning_rate * params . As pointed by [2], choosing the right learning rate can be very difficult, and even though learning rate schedules can help (annealing the learning rate using a pre-defined schedule), these schedules don&#39;t adapt as the training prgresses. . In 2015 the researcher Leslie Smith came up with the learning rate finder. The idea was to start with a very, very small learning rate and use that for one mini-batch, find what the losses are afterwards, and then increase the learning rate by some percentage (e.g., doubling it each time). Then take a pass over another mini-batch, track the loss, and double the learning rate again. This is done until the loss gets worse, instead of better. . SGD with Momentum . Momentum is one way to help SGD converge faster by accelrating in the relevant directions and reducing the oscillations (which are a result of gradients of components point in different directions). . Exponentially Moving Averages . Before we build up the equations for momentum based updates in SGD let&#39;s do a quick review of Exponentially weighted averages. Given a signal, one can compute the Exponentially weighted average given a parameter $ beta$ using the following equation:$$ mathcal{V}_{t} = beta cdot mathcal{V}_{t-1} + (1- beta) cdot mathcal{ theta}_{t} $$ Where $ mathcal{ theta}_{t}$ is the current value of the signal. We can use plot this in python code as well: . from matplotlib import pyplot as plt import seaborn as sns import numpy as np from scipy import stats sns.set_theme() def get_signal(time): x_volts = 10*np.sin(time/(2*np.pi)) x_watts = x_volts ** 2 # Calculate signal power and convert to decibles sig_avg_watts = np.mean(x_watts) return x_volts, x_watts, 10 * np.log10(sig_avg_watts) def get_noise(sig_avg_db, target_snr_db): # Calculate noise according to SNR = P_signal - P_noise # then convert to watts noise_avg_db = sig_avg_db - target_snr_db noise_avg_watts = 10 ** (noise_avg_db / 10) return 0, noise_avg_watts def moving_average(data, beta=0.9): v = [0] for idx, value in enumerate(data): v.append(beta * v[idx] + (1-beta) * value) return np.asarray(v) t = np.linspace(1, 20, 100) # Set a target SNR target_snr_db = 12 x_volts, x_watts, sig_avg_db = get_signal(t) mean_noise, noise_avg_watts = get_noise(sig_avg_db, target_snr_db) noise_volts = np.random.normal(mean_noise, np.sqrt(noise_avg_watts), len(x_volts)) # Noise up the original signal y_volts = x_volts + noise_volts plt.style.use(&#39;fivethirtyeight&#39;) # Plot signal with noise fig, ax = plt.subplots(figsize=(10,6)) for beta, color in [(0.5, &#39;black&#39;), (0.9, &#39;blue&#39;), (0.98,&#39;darkgreen&#39;)]: y_avg = moving_average(y_volts, beta=beta) ax.plot(t, y_avg[1:], label=f&#39;beta={beta}&#39;, color=color, linewidth=1.5) ax.scatter(t, y_volts, label=&#39;signal with noise&#39;, color=&#39;orange&#39;, marker=&#39;s&#39;) ax.legend(loc=&#39;upper left&#39;) ax.grid(False) plt.title(&#39;Exponentially Moving Average&#39;, fontsize=15) plt.ylabel(&#39;Signal Value&#39;, fontsize=12) plt.xlabel(&#39;Time&#39;, fontsize=12) plt.show() . . We can see the effect of the variable $ beta$​ on the smoothness of the curve, the green curve corresponding to $ beta = 0.98$​ is smoother and shifted right because it&#39;s slow to account for the change in the current signal value, recall that the equation for the Exponentially Moving Average is: $ mathcal{V}_{t} = beta cdot mathcal{V}_{t-1} + (1- beta) cdot mathcal{ theta}_{t}$​ and a higher $ beta$​ means that we&#39;re paying more attention to past values than the current one. . Now what does this have to do with SGD or Momentum? . The parameter update equation for SGD with Momentum is as follows:$$ mathcal{V}_{t} = beta cdot mathcal{V}_{t-1} + (1- beta) cdot nabla_{ mathbb{w}} E( mathbb{w}_{t}) $$ . $$ mathbb{w}_{t+1} = mathbb{w}_{t} - eta * mathcal{V}_{t} $$In some implementations we just omit the $1- beta$ term and simply use the following: . $$ mathcal{V}_{t} = beta cdot mathcal{V}_{t-1} + eta cdot nabla_{ mathbb{w}} E( mathbb{w}_{t}) $$$$ mathbb{w}_{t+1} = mathbb{w}_{t} - mathcal{V}_{t} $$where, $ mathcal{V}_{t}$ is the Exponentially Moving Average. . With momentum update like above, the parameters will build up the velocity in direction that has consistent gradient. The variable $ beta$​ can be interpreted as coefficient of friction which has a dampening effect on an object moving at a certain velocity. This variable reduces the Kinetic Energy of the system, which would otherwise never come to a stop. . Nesterov Momentum . The idea behind Nesterov Momentum is to lookahead at the value of the weights in the basin where we&#39;ll end up at if we had applied the momentum update. So, instead of using $ mathbb{w}_{t}$​​ we apply the momentum update to calculate $ mathbb{w}_{t}^{ahead} = mathbb{w}_{t} - beta cdot mathcal{V}_{t} $​​ and use this value to calculate the gradient and perform the update, reason being that we&#39;re going to land in the vicity of this point anyway, and being approximately there gives us a better estimate of the updates. . $$ mathbb{w}_{t}^{ahead} = mathbb{w}_{t} - beta cdot mathcal{V}_{t} $$$$ mathcal{V}_{t} = beta cdot mathcal{V}_{t-1} + eta cdot nabla_{ mathbb{w}} E( mathbb{w}_{t}^{ahead}) $$$$ mathbb{w}_{t+1} = mathbb{w}_{t} - mathcal{V}_{t} $$$$ $$ . Source: CS231n I&#39;ll stop here for now and talk about RMSProp, AdaGrad, Adam in a later post. . References . [1] Efficient Backprop by Yann LeCun . [2] An overview of gradient descent optimization algorithms . [3] Convolutional Neurak Networks for Visual Recognition . [4] C. Darken, J. Chang and J. Moody, &quot;Learning rate schedules for faster stochastic gradient search,&quot; Neural Networks for Signal Processing II Proceedings of the 1992 IEEE Workshop, 1992, pp. 3-12, doi: 10.1109/NNSP.1992.253713. . [5] Fastbook Chapter 5 . [6] Leslie N. Smith, Cyclical Learning Rates for Training Neural Networks, arXiv:1506.01186 . [7] Why Momentum Really Works . [8] Improving Deep Neural Networks: Hyperparameter Tuning, Regularization and Optimization . [9] Lecture Notes AA222 . [10] Linear Algebra and Optimization for Machine Learning . [11] Adding Noise to a signal in Python .",
            "url": "https://akashmehra.github.io/blog/optimization/2021/07/27/optimization.html",
            "relUrl": "/optimization/2021/07/27/optimization.html",
            "date": " • Jul 27, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "A comparison of Pets classifier using vanilla PyTorch and Fast.ai",
            "content": "Disclaimer: I am a fairly new to the library and this is just to show what I&#39;ve observed so far, I may be missing a point or two, so, don&#39;t treat the above as a comprehensive list of what fastai can do. This is just my attempt to keep learning and evolving. . Introduction . Below is generally the plan that everyone follows when it comes to training a Machine Learning model: . Load Data | Inspect Data: Plot a few examples | Create a DataLoader | Define a model architecture. | Write a training loop. | Plot metrics. | . There&#39;s also another step, which is . Analyze Errors. | . but we&#39;ll tackle this in a separate blog post once we&#39;ve covered the training of the model bit. . PyTorch Version . from torch import nn from torch import optim from torch.utils.data import dataset, dataloader from torch.autograd import Variable from torch.nn import functional as F from torchvision import datasets, transforms from torchvision.models.resnet import resnet18 from tqdm.notebook import tqdm from sklearn.model_selection import train_test_split import os from collections import Counter, OrderedDict import re import requests import tarfile . . Fetch Data . The data is a tar-gzip archive, to extract data files from it we&#39;ll use the package called tarfile. But first we need to download the archive. . The code in the cell below is taken form: https://gist.github.com/devhero/8ae2229d9ea1a59003ced4587c9cb236#gistcomment-3775721. . def fetch_data(url, data_dir, download=False): if download: response = requests.get(url, stream=True) file = tarfile.open(fileobj=response.raw, mode=&quot;r|gz&quot;) file.extractall(path=data_dir) . In the interest of comparison, I&#39;ll first write the Dataset class and see how easy it gets when we use fastai. The url that we want to fetch the data from is here: Pets Dataset . pets_url = &#39;https://s3.amazonaws.com/fast-ai-imageclas/oxford-iiit-pet.tgz&#39; data_dir = os.path.join(&#39;gdrive&#39;, &#39;MyDrive&#39;, &#39;pets_data&#39;) base_img_dir = os.path.join(data_dir, &#39;oxford-iiit-pet&#39;, &#39;images&#39;) fetch_data(pets_url, data_dir) . . We&#39;ve extracted the data in the folder named pets_data, on inspection, it looks like the folder pets_data/oxford-iiit-pet/images contains all the images we want (some files need to be filtered out as they&#39;re not in JPEG format). The filenames have the category labels in their name itself in the format: &lt;CATEGORYNAME&gt;_&lt;NUMBER&gt;.jpg. . Extract Labels . In order to extract the category name from the file names in fastbook, a RegexLabeller is used. We&#39;ll write a similar LabelExtractor (although it&#39;s very inferior in functionality to the fastai&#39;s RegexLabeller but does the job for now). . class RegexLabelExtractor(): def __init__(self, pattern): self.pattern = pattern self._names = [] def __call__(self, iterable): return [re.findall(self.pattern, value)[0] for value in iterable] . As mentioned before our version, RegexLabelExtractor extracts the label given a text. It accepts a pattern during instantiation and on __call__ it expects an iterable containing a list of texts containing the labels. It returns all the label names in a Python list . Once we&#39;ve defibed a class for extracting the labels, we&#39;d like to define a container that is responsible for maintaining a map of CATEGORYNAME -&gt; ID, which we&#39;ll use to convert the labels to an integer format and vice versa. . Below we define a LabelManager, it exposes a id_for_label* and label_for_id* methods along with keys, which returns the unique label names in our dataset (this our vocabulary size). We can also call len on a LabelManager object to know the number of output classes. . *These are a type of OrderedDict. . class LabelManager(): def __init__(self, labels): self._label_to_idx = OrderedDict() for label in labels: if label not in self._label_to_idx: self._label_to_idx[label] = len(self._label_to_idx) self._idx_to_label = {v:k for k,v in self._label_to_idx.items()} @property def keys(self): return list(self._label_to_idx.keys()) def id_for_label(self, label): return self._label_to_idx[label] def label_for_id(self, idx): return self._idx_to_label[idx] def __len__(self): return len(self._label_to_idx) . Data Splitter . We&#39;d also like to spilt our dataset into train and validation subsets. Although the dataset provides a list of train and validation splits but to be consistent with the book, we&#39;ll just write our version of the RandomSplitter (which again would be very inferior in functionality, but will do the job for the purposes of demonstration). . We&#39;d like this Splitter to accept a percentage to split on and also a seed for reproducibility. . class Splitter(): def __init__(self, valid_pct=0.2, seed = None): self.seed = seed self.valid_pct = valid_pct def __call__(self, dataset): return train_test_split(dataset, test_size=self.valid_pct, random_state=np.random.RandomState(self.seed)) . Writing a PyTorch Dataset . Now that we have a way to extract labels, maintain them in a map and split the data into train and validation splits, we&#39;ll define a PetsDataset ( a PyTorch Dataset) which will be used by the PyTorch DataLoaderto give us the data we need to provide our model to train. . A note on PyTorch Dataset: A PyTorch dataset is a primitive provided by the library that stores the samples and their corresponding labels. In order to write a custom dataset, our class PetsDataset needs to implement three functions: __init__, __len__, and __getitem__. . class PetsDataset(dataset.Dataset): def __init__(self, data, tfms=None): super(PetsDataset, self).__init__() self.data = data self.transforms = tfms def __getitem__(self, idx): X = Image.open(self.data[idx][0]) if X.mode != &#39;RGB&#39;: X = X.convert(&#39;RGB&#39;) y = self.data[idx][1] if self.transforms: X = self.transforms(X) return (X, y) def __len__(self): return len(self.data) . Notice how we&#39;re opening the Image only when __getitem__ is called and we also have to make sure that all the images have 3 input channels, hence the check if X.mode != &#39;RGB&#39;. Some images in the dataset have this issue and if we don&#39;t convert them to have 3 input channels then the DataLoader wouldn&#39;t be able to create a batch using torch.stack . We&#39;re now ready to use these datasets, but we&#39;ll need to make sure that our global map of CATEGORYNAME -&gt; ID is constructed using both the train and the validation splits, we&#39;ll also have this class hold our corresponding datasets. . class DatasetManager(): def __init__(self, base_dir, paths, label_extractor, tfms=None, valid_pct=0.2, seed=None): self._labels = label_extractor(paths) self.tfms = tfms self._label_manager = LabelManager(self._labels) self._label_ids = [self.label_manager.id_for_label(label) for label in self._labels] self.abs_paths = [os.path.join(base_dir, path) for path in paths] self.train_data, self.valid_data = Splitter(valid_pct=valid_pct, seed=seed)(list(zip(self.abs_paths, self._label_ids))) @property def label_manager(self): return self._label_manager @property def train_dataset(self): return PetsDataset(self.train_data, tfms=self.tfms) @property def valid_dataset(self): return PetsDataset(self.valid_data, tfms=self.tfms) . We&#39;ll now use all the helper classes we&#39;ve created so far to use the datasets in a dataloader and look at the plan to choose an architecture and train it (almost there). . paths = [path for path in sorted(os.listdir(base_img_dir)) if path.endswith(&#39;.jpg&#39;)] pattern = &#39;(.+)_ d+.jpg$&#39; regex_label_extractor = RegexLabelExtractor(pattern) dataset_manager = DatasetManager(base_img_dir, paths, regex_label_extractor, tfms=transforms.Compose([transforms.Resize((224, 224)), transforms.ToTensor()]), seed=42) train_dataset = dataset_manager.train_dataset valid_dataset = dataset_manager.valid_dataset . Before we look at the model, let&#39;s just for sake of sanity look at the labels we&#39;re dealing with and possibly plot a few images. This step is just to make sure that things are working as expected and the dataloader will be batching the data in the right way and our Trainer won&#39;t crash midway. . df = pd.DataFrame(dataset_manager.label_manager.keys, columns=[&#39;label_name&#39;]) df.head(len(df)) . label_name . 0 Abyssinian | . 1 Bengal | . 2 Birman | . 3 Bombay | . 4 British_Shorthair | . 5 Egyptian_Mau | . 6 Maine_Coon | . 7 Persian | . 8 Ragdoll | . 9 Russian_Blue | . 10 Siamese | . 11 Sphynx | . 12 american_bulldog | . 13 american_pit_bull_terrier | . 14 basset_hound | . 15 beagle | . 16 boxer | . 17 chihuahua | . 18 english_cocker_spaniel | . 19 english_setter | . 20 german_shorthaired | . 21 great_pyrenees | . 22 havanese | . 23 japanese_chin | . 24 keeshond | . 25 leonberger | . 26 miniature_pinscher | . 27 newfoundland | . 28 pomeranian | . 29 pug | . 30 saint_bernard | . 31 samoyed | . 32 scottish_terrier | . 33 shiba_inu | . 34 staffordshire_bull_terrier | . 35 wheaten_terrier | . 36 yorkshire_terrier | . . Data Inspection . A method to plot one batch of data (inspired by fastai of course but again a very curtailed version of what that function does). Notice how we&#39;re calling transforms.ToPILImage(), that&#39;s because we have objects of type torch.Tensor in our batch and in order to plot them we need to convert them to a PIL.Image, rest everything is done to just make sure we&#39;ve got the images aligned in a nice way across different panels. . def plot_one_batch(batch, max_images=9): nrows = int(math.sqrt(max_images)) ncols = int(math.sqrt(max_images)) if nrows * ncols != max_images: nrows = (max_images + ncols - 1) // ncols fig, ax = plt.subplots(nrows=nrows, ncols=ncols, figsize=(20, 10)) X,Y = next(batch) for idx, x in enumerate(X[:max_images]): y = Y[idx] ax.ravel()[idx].imshow(transforms.ToPILImage()(x)) ax.ravel()[idx].set_title(f&#39;{y}/{dataset_manager.label_manager.label_for_id(y.item())}&#39;) ax.ravel()[idx].set_axis_off() plt.tight_layout() plt.show() . This function generates one batch of data given a dataloader, this is just using Python Generators . def generate_one_batch(dl): for batch in dl: yield batch . plot_one_batch(generate_one_batch(train_dl), max_images=20) . Model Architecture . Now, we&#39;re ready to look at the model and make a few decisions about the architecture we want to use. . Here&#39;s our requirement: We want to extract the features from an image and then uses a classification head to get the output distribution over the number of classes (our labels from before). We&#39;ll define a loss and use it to optimize the network. . Because we&#39;re dealing with images, a Convolution Neural Network (CNN) seems like a good start, in the literature as well as fastbook, a restnet type architecture has been used, so let&#39;s use that and see what we can do with it. . Coding a ResNet is a separate blog post on its own, so, we&#39;ll punt that for now and use what&#39;s available to us in the form a pretrained model. . model = resnet34(pretrained=True, progress=True) . Downloading: &#34;https://download.pytorch.org/models/resnet34-b627a593.pth&#34; to /root/.cache/torch/hub/checkpoints/resnet34-b627a593.pth . . Changing the classifier . Since this model is trained to give an output distribution for 1000 classes, we can just change that layer to give us an output distribution based on what we have in our dataset and then fine-tune this layer. To read more on fine-tuning refer to fastbook . model.fc = nn.Linear(512,len(dataset_manager.label_manager), bias=True) . Making the model fine-tunable . We&#39;ll freeze all the layers of the model except for the fc classification head we added above. . def make_fine_tunable(model): for param in model.parameters(): param.requires_grad = False for param in model.fc.parameters(): param.requires_grad = True print(&quot;Tunable Layers: &quot;) for (name, param) in model.named_parameters(): if param.requires_grad: print(f&#39;{name} -&gt; {param.requires_grad}&#39;) . make_fine_tunable(model) . Tunable Layers: fc.weight -&gt; True fc.bias -&gt; True . . Trainer . Now comes in a point where we have to write a traininig loop and this is where things get into the Boiler Plate code category even more. We shouldn&#39;t be writing this but that&#39;s the point of this blog post that using fastai we can offload a lot of the boiler plate code to the library and use the goodies offered by the library to our advantage and focus more on research/modeling. . We are maintaining an instance of model, criterion, an optimizer and dataloaders. We step through a batch during train_epoch and incur a loss. We use this loss to make a backward pass and let the optimizer take a step by updating the network parameters. We also have a validate function that calculates loss and accuracy on validation dataset after every epoch . class Trainer(): def __init__(self, train_dataloader, model, criterion, optimizer, test_dataloader=None): self.train_dl = train_dataloader self.model = model self.test_dl = test_dataloader self.criterion = criterion self.optimizer = optimizer self.recorder = {&#39;loss&#39;: { &#39;train&#39;: {}, &#39;test&#39;: {}} , &#39;accuracy&#39;: {&#39;train&#39;: {}, &#39;test&#39;: {}}} def step_batch(self, X,y): X = X.cuda() y = y.cuda() logits = self.model(X) loss = self.criterion(logits, y) probs = F.softmax(logits, dim=1) return loss, logits, probs def train_epoch(self, epoch): self.model.train() running_loss = 0 for X,y in tqdm(self.train_dl, leave=False): self.optimizer.zero_grad() loss, _, _ = self.step_batch(X,y) running_loss += loss loss.backward() self.optimizer.step() epoch_loss = running_loss / len(self.train_dl) self.recorder[&#39;loss&#39;][&#39;train&#39;][epoch] = epoch_loss return epoch_loss @torch.no_grad() def accuracy(self): correct = 0 total = 0 for X,y in tqdm(self.test_dl): total += y.size(0) logits = model(X) probs = F.softmax(logits, dim=1) _, y_pred = torch.max(probs, dim=1) correct += (y_pred == y).sum() acc = correct / float(total) return acc @torch.no_grad() def validate(self, epoch): running_loss = 0 total = 0 correct = 0 for X,y in tqdm(self.test_dl, leave=False): y = y.cuda() total += y.size(0) loss, logits, probs = self.step_batch(X,y) running_loss += loss _, y_pred = torch.max(probs, dim=1) correct += (y_pred == y).cpu().sum() acc = correct / float(total) epoch_loss = running_loss / len(self.test_dl) self.recorder[&#39;loss&#39;][&#39;test&#39;][epoch] = epoch_loss self.recorder[&#39;accuracy&#39;][&#39;test&#39;][epoch] = acc return epoch_loss, acc def train(self, num_epochs): for epoch in tqdm(range(num_epochs), leave=False): train_loss = self.train_epoch(epoch) test_loss, test_acc = self.validate(epoch) #print(f&quot;Training Loss: {train_loss}, tTest Loss: {test_loss}, tTest Accuracy: {test_acc}&quot;) . Training (fine-tuning) the model . Let&#39;s send the model over to the GPU for faster training. . model = model.cuda() . Hyperparameters . Let&#39;s define a configuration that will hold our hyper-parameters . class TrainConfig(): def __init__(self, bs=32, lr=1e-2, seed=42, betas=(0.9, 0.999), num_workers=4): self.bs = bs self.lr = lr self.seed = seed self.betas = betas self.num_workers = num_workers . We set the seed for reproducibility, and instantiate dataloader objects. Notice how we&#39;re using &gt; 1 num_workers. That speeds up the data loading process. . config = TrainConfig(bs=128) torch.manual_seed(config.seed) train_dl = dataloader.DataLoader(train_dataset, batch_size=config.bs, shuffle=True, num_workers=config.num_workers) valid_dl = dataloader.DataLoader(valid_dataset, batch_size=config.bs, shuffle=False, num_workers=config.num_workers) . We define our criterion as nn.CrossEntropy and choose our optimizer to be an instance of optim.Adam, after that we instantiate our trainer object and train (fine-tune in our case) for a few epochs. . Criterion, Optimizer and Training . criterion = nn.CrossEntropyLoss() optimizer = optim.Adam(model.parameters(), lr=1e-3, betas=(0.9, 0.999)) trainer = Trainer(train_dl, model, criterion, optimizer, test_dataloader=valid_dl) trainer.train(10) . Plotting Utilities . Helper functions for plotting our loss and accuracies (which we&#39;ve recorded using our trainer) . def plot_losses(losses): train_loss = losses[&#39;train&#39;] test_loss = losses[&#39;test&#39;] plt.style.use(&#39;fivethirtyeight&#39;) fig, ax = plt.subplots(figsize=(7, 4)) ax.plot(train_loss, color=&#39;blue&#39;, label=&#39;Training Loss&#39;) ax.plot(test_loss, color=&#39;green&#39;, label=&#39;Test Loss&#39;) ax.set(title=&quot;Loss over epochs&quot;, xlabel=&quot;Epochs&quot;, ylabel=&quot;Loss&quot;) ax.legend() fig.show() plt.style.use(&#39;default&#39;) . def plot_accuracy(accuracy): plt.style.use(&#39;fivethirtyeight&#39;) fig, ax = plt.subplots(figsize=(7, 4)) ax.plot(accuracy, color=&#39;blue&#39;, label=&#39;Test Accuracy&#39;) ax.set(title=&quot;Accuracy over epochs&quot;, xlabel=&quot;Epochs&quot;, ylabel=&quot;Accuracy&quot;) ax.legend() fig.show() plt.style.use(&#39;default&#39;) . Loss . losses = { k: np.asarray([t.item() for t in v.values()]) for k,v in trainer.recorder[&#39;loss&#39;].items() } plot_losses(losses) . Accuracy . accuracies = { k: np.asarray([t.item() for t in v.values()]) for k,v in trainer.recorder[&#39;accuracy&#39;].items()} plot_accuracy(accuracy=accuracies[&#39;test&#39;]) . Summary of PyTorch Version . Load Data | Inspect Data: Plot a few examples | Create a DataLoader | Define a model architecture. | Write a training loop. | Plot metrics. | Fast.ai Version . Now let&#39;s see how this can done using fastai. One could treat the following cells as a completely different notebook altogether. . from fastcore.all import L from fastai.vision.all import * matplotlib.rc(&#39;image&#39;, cmap=&#39;Greys&#39;) . . Fetch Data . We&#39;ll first download the Pets data and untar it using the untar_data function and this function really takes care of filtering the images and storing them somewhere on the disk for us and then returning the paths. It&#39;s helpful as I don&#39;t have to take a peek at the response object and parse it then untar it, apply filters and then iterate through the directory. This function does it all for us. To know more about untar_data, please checkout the documentation for untar_data . path = untar_data(URLs.PETS) . Path.BASE_PATH = path path.ls() . (#2) [Path(&#39;annotations&#39;),Path(&#39;images&#39;)] . . (path/&quot;images&quot;).ls() . (#7393) [Path(&#39;images/great_pyrenees_160.jpg&#39;),Path(&#39;images/shiba_inu_82.jpg&#39;),Path(&#39;images/scottish_terrier_2.jpg&#39;),Path(&#39;images/Russian_Blue_144.jpg&#39;),Path(&#39;images/pomeranian_166.jpg&#39;),Path(&#39;images/english_cocker_spaniel_48.jpg&#39;),Path(&#39;images/japanese_chin_180.jpg&#39;),Path(&#39;images/scottish_terrier_15.jpg&#39;),Path(&#39;images/Sphynx_166.jpg&#39;),Path(&#39;images/Maine_Coon_98.jpg&#39;)...] . . Define DataBlock . Let&#39;s construct a DataBlock object. A DataBlock object provides us encapsulation over many aspects of our data loading and arranging pipeline. It let&#39;s us define the . blocks which make up for X and y in our dataset . This will also automtically convert the labels to integer ids | . | extract the label from the name attribute of the file | apply Transformations for us which can help us do Data Augmentation and resizing in one go. | Randomly split the data into training and validation splits. | . Notice how it does all the work and more (we didn&#39;t do any augmentation) of the classes Splitter, RegexLabelExtractor, LabelManager, DatasetManager defined above in just one call, and since it&#39;s well maintained, offers us much more functionality, generic, more performamnt, well tested and maintained, we don&#39;t need to keep writing our own versions from scratch every time we are tasked with training a classifier. . pets = DataBlock(blocks = (ImageBlock, CategoryBlock), get_items=get_image_files, splitter=RandomSplitter(seed=42), get_y=using_attr(RegexLabeller(r&#39;(.+)_ d+.jpg$&#39;), &#39;name&#39;), item_tfms=Resize(460), batch_tfms=aug_transforms(size=224, min_scale=0.75)) . To check if everything will work fine, there&#39;s a pretty handy function called summary that we call on the DataBlock object that will show us the whole plan and will tell us a meningful error message if there&#39;s an issue with our pipeline somewhere. . Sanity Check . pets.summary(path/&quot;images&quot;) . Setting-up type transforms pipelines Collecting items from /root/.fastai/data/oxford-iiit-pet/images Found 7390 items 2 datasets of sizes 5912,1478 Setting up Pipeline: PILBase.create Setting up Pipeline: partial -&gt; Categorize -- {&#39;vocab&#39;: None, &#39;sort&#39;: True, &#39;add_na&#39;: False} Building one sample Pipeline: PILBase.create starting from /root/.fastai/data/oxford-iiit-pet/images/Persian_180.jpg applying PILBase.create gives PILImage mode=RGB size=334x500 Pipeline: partial -&gt; Categorize -- {&#39;vocab&#39;: None, &#39;sort&#39;: True, &#39;add_na&#39;: False} starting from /root/.fastai/data/oxford-iiit-pet/images/Persian_180.jpg applying partial gives Persian applying Categorize -- {&#39;vocab&#39;: None, &#39;sort&#39;: True, &#39;add_na&#39;: False} gives TensorCategory(7) Final sample: (PILImage mode=RGB size=334x500, TensorCategory(7)) Collecting items from /root/.fastai/data/oxford-iiit-pet/images Found 7390 items 2 datasets of sizes 5912,1478 Setting up Pipeline: PILBase.create Setting up Pipeline: partial -&gt; Categorize -- {&#39;vocab&#39;: None, &#39;sort&#39;: True, &#39;add_na&#39;: False} Setting up after_item: Pipeline: Resize -- {&#39;size&#39;: (460, 460), &#39;method&#39;: &#39;crop&#39;, &#39;pad_mode&#39;: &#39;reflection&#39;, &#39;resamples&#39;: (2, 0), &#39;p&#39;: 1.0} -&gt; ToTensor Setting up before_batch: Pipeline: Setting up after_batch: Pipeline: IntToFloatTensor -- {&#39;div&#39;: 255.0, &#39;div_mask&#39;: 1} -&gt; Flip -- {&#39;size&#39;: None, &#39;mode&#39;: &#39;bilinear&#39;, &#39;pad_mode&#39;: &#39;reflection&#39;, &#39;mode_mask&#39;: &#39;nearest&#39;, &#39;align_corners&#39;: True, &#39;p&#39;: 0.5} -&gt; RandomResizedCropGPU -- {&#39;size&#39;: (224, 224), &#39;min_scale&#39;: 0.75, &#39;ratio&#39;: (1, 1), &#39;mode&#39;: &#39;bilinear&#39;, &#39;valid_scale&#39;: 1.0, &#39;max_scale&#39;: 1.0, &#39;p&#39;: 1.0} -&gt; Brightness -- {&#39;max_lighting&#39;: 0.2, &#39;p&#39;: 1.0, &#39;draw&#39;: None, &#39;batch&#39;: False} Building one batch Applying item_tfms to the first sample: Pipeline: Resize -- {&#39;size&#39;: (460, 460), &#39;method&#39;: &#39;crop&#39;, &#39;pad_mode&#39;: &#39;reflection&#39;, &#39;resamples&#39;: (2, 0), &#39;p&#39;: 1.0} -&gt; ToTensor starting from (PILImage mode=RGB size=334x500, TensorCategory(7)) applying Resize -- {&#39;size&#39;: (460, 460), &#39;method&#39;: &#39;crop&#39;, &#39;pad_mode&#39;: &#39;reflection&#39;, &#39;resamples&#39;: (2, 0), &#39;p&#39;: 1.0} gives (PILImage mode=RGB size=460x460, TensorCategory(7)) applying ToTensor gives (TensorImage of size 3x460x460, TensorCategory(7)) Adding the next 3 samples No before_batch transform to apply Collating items in a batch Applying batch_tfms to the batch built Pipeline: IntToFloatTensor -- {&#39;div&#39;: 255.0, &#39;div_mask&#39;: 1} -&gt; Flip -- {&#39;size&#39;: None, &#39;mode&#39;: &#39;bilinear&#39;, &#39;pad_mode&#39;: &#39;reflection&#39;, &#39;mode_mask&#39;: &#39;nearest&#39;, &#39;align_corners&#39;: True, &#39;p&#39;: 0.5} -&gt; RandomResizedCropGPU -- {&#39;size&#39;: (224, 224), &#39;min_scale&#39;: 0.75, &#39;ratio&#39;: (1, 1), &#39;mode&#39;: &#39;bilinear&#39;, &#39;valid_scale&#39;: 1.0, &#39;max_scale&#39;: 1.0, &#39;p&#39;: 1.0} -&gt; Brightness -- {&#39;max_lighting&#39;: 0.2, &#39;p&#39;: 1.0, &#39;draw&#39;: None, &#39;batch&#39;: False} starting from (TensorImage of size 4x3x460x460, TensorCategory([ 7, 22, 6, 14], device=&#39;cuda:0&#39;)) applying IntToFloatTensor -- {&#39;div&#39;: 255.0, &#39;div_mask&#39;: 1} gives (TensorImage of size 4x3x460x460, TensorCategory([ 7, 22, 6, 14], device=&#39;cuda:0&#39;)) applying Flip -- {&#39;size&#39;: None, &#39;mode&#39;: &#39;bilinear&#39;, &#39;pad_mode&#39;: &#39;reflection&#39;, &#39;mode_mask&#39;: &#39;nearest&#39;, &#39;align_corners&#39;: True, &#39;p&#39;: 0.5} gives (TensorImage of size 4x3x460x460, TensorCategory([ 7, 22, 6, 14], device=&#39;cuda:0&#39;)) applying RandomResizedCropGPU -- {&#39;size&#39;: (224, 224), &#39;min_scale&#39;: 0.75, &#39;ratio&#39;: (1, 1), &#39;mode&#39;: &#39;bilinear&#39;, &#39;valid_scale&#39;: 1.0, &#39;max_scale&#39;: 1.0, &#39;p&#39;: 1.0} gives (TensorImage of size 4x3x224x224, TensorCategory([ 7, 22, 6, 14], device=&#39;cuda:0&#39;)) applying Brightness -- {&#39;max_lighting&#39;: 0.2, &#39;p&#39;: 1.0, &#39;draw&#39;: None, &#39;batch&#39;: False} gives (TensorImage of size 4x3x224x224, TensorCategory([ 7, 22, 6, 14], device=&#39;cuda:0&#39;)) . . Dataloader . Let&#39;s define our dataloader. . dls = pets.dataloaders(path/&quot;images&quot;) . Training (fine-tuning) the model . Let&#39;s train our model for two epochs using cnn_learner, the model we&#39;ll use is resnet34 . learn = cnn_learner(dls, resnet34, metrics=error_rate) learn.fine_tune(10) . epoch train_loss valid_loss error_rate time . 0 | 1.492460 | 0.365994 | 0.117727 | 01:09 | . epoch train_loss valid_loss error_rate time . 0 | 0.497617 | 0.326611 | 0.109608 | 01:14 | . 1 | 0.303163 | 0.237988 | 0.077131 | 01:14 | . Notice how we didn&#39;t have to worry about sending the data or the model over to the GPU. . Interpretation and Analysis . And the cherry on top is the ability to do interpretation and analyze errors with a very neatly written function call. . interp = ClassificationInterpretation.from_learner(learn) interp.plot_confusion_matrix(figsize=(12,12), dpi=60) . . Summary . And we&#39;re done! Sure the model can be improved upon from here, but the point is that I can now focus on that bit precisely after just getting started and not worry about anything else. I&#39;d advice now to please read the chapter 5 of the fastbook as the last few lines have missed a few points about Data Augmentation, finding the right Learning Rate etc. . References . Fastai docs | Fastbook Chapter 5 | Weights &amp; Biases forum for chapter 5 | Implementing Yann LeCun’s LeNet-5 in PyTorch | How can I disable all layers gradient expect the last layer in Pytorch? | Grayscale to RGB transform | PyTorch PIL to Tensor and vice versa | Deep residual Learning for Image Recognition | .",
            "url": "https://akashmehra.github.io/blog/classification/pytorch/fastai/2021/07/20/pets_classifier.html",
            "relUrl": "/classification/pytorch/fastai/2021/07/20/pets_classifier.html",
            "date": " • Jul 20, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "I am a Machine Learning Engineer. This is my personal blog1 where I write about my learnings in the field of Machine Learning and Artifical Intelligence. I am very passionate about math and its applications and usually find myself reading math theory in my free time. I love playing football and cricket when I am not working, reading, sleeping or working out. . [1] The views expressed here are mine alone and not those of my employer. .",
          "url": "https://akashmehra.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://akashmehra.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}